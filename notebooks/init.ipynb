{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc91ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8717141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "lm = dspy.LM(\"ollama_chat/gpt-oss:20b\", api_base=\"http://localhost:11434\", api_key=\"fake\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae7516",
   "metadata": {},
   "source": [
    "To invoke the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb49a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There are **3** 'r's in the word *strawberry*.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(messages=[{\"role\": \"user\", \"content\": \"Hi! How many 'r's are there in strawberry?\"}])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbcfcb",
   "metadata": {},
   "source": [
    "# 1. Inline signatures\n",
    "\n",
    "Declare signatures inline using strings and arrows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4231db3",
   "metadata": {},
   "source": [
    "## Chain Of Thought\n",
    "GPT-oss has a 128k context window! Let's make it summarize some documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1780c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"../docs\"):\n",
    "    os.makedirs(\"../docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b65dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-09 21:48:10--  https://arxiv.org/pdf/2505.20286\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1113373 (1.1M) [application/pdf]\n",
      "Saving to: ‘../docs/alita_paper.pdf’\n",
      "\n",
      "../docs/alita_paper 100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-08-09 21:48:10 (8.36 MB/s) - ‘../docs/alita_paper.pdf’ saved [1113373/1113373]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://arxiv.org/pdf/2505.20286 -O \"../docs/alita_paper.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84ea038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "docs = SimpleDirectoryReader(\"../docs\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9981cc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Doc ID: 61884915-871e-40fd-a2f2-28b59641dd22\n",
       "Text: arXiv:2505.20286v1  [cs.AI]  26 May 2025 ALITA : G ENERALIST\n",
       "AGENT ENABLING SCALABLE AGENTIC REASONING WITH MINIMAL PREDEFINITION\n",
       "AND MAXIMAL SELF -EVOLUTION Jiahao Qiu∗1, Xuan Qi∗2, Tongcheng\n",
       "Zhang∗3, Xinzhe Juan3,4, Jiacheng Guo1, Yifu Lu1, Yimin Wang3,4, Zixin\n",
       "Yao1, Qihan Ren3, Xun Jiang5, Xing Zhou5, Dongrui Liu3, Ling Yang1,\n",
       "Yue Wu1, Kaixua..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(str(docs[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50f5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e89b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize = dspy.ChainOfThought('full_document -> summary')\n",
    "response = summarize(full_document = doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0a6e435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alita successfully generated a YouTube Video Subtitle Crawler MCP, executed it to retrieve the transcript of the specified 360 VR video, and extracted the correct number “100000000” mentioned by the narrator after the dinosaur scene. The workflow involved MCP brainstorming, web search for an open‑source tool, environment setup, code generation, MCP packaging, and final answer extraction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7681a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The case study demonstrates Alita’s workflow for extracting a specific piece of information from a YouTube 360 VR video. The process begins with an MCP Brainstorming step, where Alita identifies the need for a “YouTube Video Subtitle Crawler” MCP to automate subtitle extraction. The Web Agent then searches open‑source repositories and locates the `youtube-transcript-api` library on GitHub. The Manager Agent synthesizes this information, writes a Python function that uses the API to fetch the transcript, and generates environment setup instructions (conda environment creation and pip install). Once the code is executed in the prepared environment, the Manager Agent packages the function into the MCP, which is then used to scrape the subtitles from the target video. By parsing the transcript, Alita identifies the number “100000000” mentioned immediately after the dinosaurs are first shown. This answer matches the correct answer provided in the dataset."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.reasoning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c245e438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-09T21:48:11.903709]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `full_document` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `summary` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## full_document ## ]]\n",
      "{full_document}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## summary ## ]]\n",
      "{summary}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `full_document`, produce the fields `summary`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## full_document ## ]]\n",
      "arXiv:2505.20286v1  [cs.AI]  26 May 2025\n",
      "ALITA : G ENERALIST AGENT ENABLING SCALABLE AGENTIC\n",
      "REASONING WITH MINIMAL PREDEFINITION AND MAXIMAL\n",
      "SELF -EVOLUTION\n",
      "Jiahao Qiu∗1, Xuan Qi∗2, Tongcheng Zhang∗3, Xinzhe Juan3,4, Jiacheng Guo1, Yifu Lu1, Yimin Wang3,4, Zixin Yao1,\n",
      "Qihan Ren3, Xun Jiang5, Xing Zhou5, Dongrui Liu3, Ling Yang1, Yue Wu1, Kaixuan Huang1, Shilong Liu1,\n",
      "Hongru Wang6, Mengdi Wang1\n",
      "1AI Lab, Princeton University 2IIIS, Tsinghua University 3Shanghai Jiao Tong University\n",
      "4University of Michigan 5Tianqiao and Chrissy Chen Institute 6The Chinese University of Hong Kong\n",
      "0.3 0.4 0.5 0.6 0.7 0.8 0.9\n",
      "Average\n",
      "Level 3\n",
      "Level 2\n",
      "Level 1\n",
      "87.3%\n",
      "76.9%\n",
      "89.5%\n",
      "88.7%\n",
      "73.3%\n",
      "57.7%\n",
      "70.1%\n",
      "86.5%\n",
      "67.4%\n",
      "47.6%\n",
      "69.1%\n",
      "74.3%\n",
      "GAIA Benchmark\n",
      "Alita manus.ai OpenAI DeepResearch\n",
      "Figure 1: Performance of Alita, manus.ai, and OpenAI DeepResearch[1]\n",
      "ABSTRACT\n",
      "Recent advances in large language models (LLMs) have enabled agents to autonomously perform\n",
      "complex, open-ended tasks. However, many existing frameworks depend heavily on manually\n",
      "predefined tools and workflows, which hinder their adaptability, scalability, and generalization\n",
      "across domains. In this work, we introduce Alita—a generalist agent designed with the principle\n",
      "of \"Simplicity is the ultimate sophistication,\" enabling scalable agentic reasoning through minimal\n",
      "predefinition and maximal self-evolution. For minimal predefinition, Alita is equipped with only one\n",
      "component for direct problem-solving, making it much simpler and neater than previous approaches\n",
      "that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances\n",
      "its potential to generalize to challenging questions, without being limited by tools. For Maximal\n",
      "self-evolution, we enable the creativity of Alita by providing a suite of general-purpose components\n",
      "to autonomously construct, refine, and reuse external capabilities by generating task-related model\n",
      "context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably,\n",
      "Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-\n",
      "purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively,\n",
      "on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. More\n",
      "details will be updated at https://github.com/CharlesQ9/Alita.\n",
      "∗ These authors contributed equally to this work.\n",
      "\n",
      "1 Introduction\n",
      "\"Simplicity is the ultimate sophistication.\" — Leonardo da Vinci\n",
      "Large language models (LLMs) have rapidly evolved from merely generating text to autonomous agents capable of\n",
      "independently planning and executing complex tasks on behalf of users with limited human oversight [ 2]. These\n",
      "capabilities have enabled a wide range of applications, ranging from travel planning [ 3], computer use [ 4, 5, 6],\n",
      "to the multi-step research tasks [ 7]. To support such diverse and demanding tasks, a new class of systems called\n",
      "generalist agents has emerged. These agents are designed to handle a wide range of domains and tasks through a unified\n",
      "architecture, allowing them to generalize beyond task-specific solutions, such as OpenAI Deep Research [1] and Manus.\n",
      "However, most of the current general-purpose agents heavily rely on large-scale manual engineering, including tediously\n",
      "designed workflows, considerable pre-defined tools, and hardcoded components [8, 9]. This reliance introduces several\n",
      "critical limitations: i) It is impractical, if not impossible, to predefine all the tools required for the wide variety of\n",
      "real-world tasks an agent might encounter (imcomplete coverage); ii) Many complex tasks require agents to creatively\n",
      "compose new tools or leverage existing ones in novel ways while pre-designed workflow and hardcoded components\n",
      "constrain this compositional flexibility and inhibit the development of adaptive behaviors ( limited creativity and\n",
      "flexibility); iii) It is not always the case that the interface or environment of different tools are compatible with the agent\n",
      "(mismatch). For example, many useful tools are not written in Python, which makes it difficult, though not entirely\n",
      "impossible, for them to be pre-connected to the mainstream agent frameworks that are primarily written in Python.\n",
      "Together, these challenges ultimately hinder the scalability, adaptability, and generalization of existing generalist agents.\n",
      "In contrast to the prevailing trend of growing complexity, we propose a radically simple design philosophy built on two\n",
      "principles: i) Minimal Predefinition: Equip the agent with only a minimal set of core capabilities, avoiding manually\n",
      "engineered components for specific tasks or modalities; ii)Maximal Self-Evolution: Empower the agent to autonomously\n",
      "create, refine, and reuse external capabilities as needed. We instantiate this vision through Alita, a generalist agent built\n",
      "with a single core capability (i.e., the web agent) and a small set of general-purpose modules that enable self-directed\n",
      "capability expansion. Specifically, we take advantage of the Model Context Protocols (MCPs) 1 which is an open\n",
      "protocol that standardizes how different systems provide context to LLMs, and empower Alita to dynamically generate,\n",
      "adapt, and reuse MCPs based on the demands of each task rather than relying on static, predefined tools. This shift from\n",
      "manually designed capabilities to on-the-fly MCP construction unlocks a new path for building agents that are simple\n",
      "yet profoundly capable.\n",
      "Youtube \n",
      "Caption \n",
      "Crawler\n",
      "Image \n",
      "CaptionerUrl Text \n",
      "Extractor\n",
      "Path \n",
      "Generalist \n",
      "Classifier\n",
      "Relevant \n",
      "Patch \n",
      "Zoomer\n",
      "Web Agent MCP Box\n",
      "Alita\n",
      "(Ours)\n",
      "Minimal Predefinition\n",
      "Maximal Self-Evolution\n",
      "Self Evolving\n",
      "Other\n",
      "Agents\n",
      " \n",
      "Manager Agent\n",
      "Manager Agent\n",
      "MCP Creation\n",
      "Large-scale Manual \n",
      "Engineering\n",
      "Traditional \n",
      "Generalist\n",
      "Agents\n",
      "Scalable Dynamic Capability\n",
      "Enhanced Creativity & Flexibility\n",
      "Cross-ecosystem Compatibility\n",
      "Incomplete Coverage\n",
      "Limited Creativity & Flexibility\n",
      "Mismatch\n",
      "Web \n",
      "Agent\n",
      "Figure 2: Comparison between Traditional Generalist Agents and Alita. Traditional generalist agents heavily rely on\n",
      "large-scale manual engineering while Alita adheres to minimal predefinition and maximal self-evolution.\n",
      "We conduct comprehensive experiments on several benchmarks to assess Alita in real-world applications, especially\n",
      "on the most popular GAIA [10]. Alita proves that simplicity is not a constraint, but a strength, and that creative agent\n",
      "behavior can emerge from a design that prioritizes autonomy over manual engineering. To sum up, our key contributions\n",
      "can be summarized as follows.\n",
      "• We propose a new agent architecture centered on minimal predefinition and maximal self-evolution, challenging\n",
      "conventional design norms in generalist agents, aiming to call for a more scalable and generalizable agent\n",
      "framework.\n",
      "1https://www.anthropic.com/news/model-context-protocol\n",
      "2\n",
      "\n",
      "• We present Alita, a generalist agent that achieves scalable agentic reasoning with a radically simple design.\n",
      "• We empirically demonstrate that Alita, despite using no complex predefined tools and workflows for specific\n",
      "tasks, outperforms many systems with significantly more handcrafted complexity on the GAIA benchmark.\n",
      "We achieve 75.15% pass@1 and 87.27% pass@3, surpassing OpenAI’s Deep Research with 67.36% pass@1\n",
      "and ranking top among all general-purpose agents.\n",
      "2 Related Works\n",
      "2.1 Generalist Agent\n",
      "The concept of a Generalist Agent aims to construct an AI agent system capable of collaboratively completing a variety\n",
      "of complex tasks in a real-world environment. OWL [ 8] introduces a method that decomposes complex tasks into\n",
      "subtasks and dynamically allocates them to worker nodes with specialized toolkits. Omne [ 11] proposes a multi-\n",
      "agent collaborative development framework, where each agent possesses an independent system structure, enabling\n",
      "autonomous learning and the storage of a comprehensive world model to build an independent understanding of the\n",
      "environment. OpenAI Deep Research 2 employs reinforcement learning for training on real-world tasks, aiming to\n",
      "provide precise and reliable research reports for knowledge-intensive tasks. A-World [12] offers a highly configurable,\n",
      "modular, and scalable simulation environment, allowing developers to flexibly define and integrate different types of AI\n",
      "agents. The Magentic-One [ 13] framework merges the Magentic and Autogen systems, distinguishing between the\n",
      "micro-level LLM-driven function generation and the macro-level multi-agent orchestration, resulting in a clearer and\n",
      "more efficient construction of agent systems. Alita, also a Generalist Agent, allows for the minimal use of predefined\n",
      "tools and workflows for direct problem-solving, yet still achieves impressive performance across diverse tasks.\n",
      "2.2 Auto Generating Agent\n",
      "Auto-generating agents aim to enhance the versatility of agents by enabling them to autonomously generate tools,\n",
      "agents, or workflows tailored to specific tasks. AutoAgents [14], for instance, generate multiple agents, each playing a\n",
      "distinct role, to handle the corresponding subtasks. OpenHands [ 15] offers an event-driven architecture that allows\n",
      "agents to interact with the environment like human developers, thereby enabling the creation of custom workflows.\n",
      "AFlow [16] redefines workflow optimization as a search problem, where each workflow consists of several nodes\n",
      "invoking large LLMs, and the optimal workflow is identified and executed through a search process. AutoAgent [17], as\n",
      "an autonomous agent operating system, permits agents to manage system-level operations and file data autonomously.\n",
      "In Alita, agents are empowered to automatically generate diverse, specialized, and highly accurate MCPs to aid in the\n",
      "completion of specific tasks, while also providing resources for future executions.\n",
      "2.3 Tool Creation\n",
      "Tool Creation enables agents to autonomously create tools to assist in task execution, either on their own or with external\n",
      "support. CRAFT [18] utilizes GPT-4 to generate a set of code snippets that function as tools, which are then retrieved\n",
      "and used in the system. TroVE [19] maintains a collection of high-level functions, which are automatically generated,\n",
      "extended, and periodically pruned to optimize program generation. CREATOR [20] decouples the abstraction of tool\n",
      "creation from the actual execution, allowing LLMs to address tasks at different levels of granularity. AutoAgent [17]\n",
      "enables agents to autonomously create new tools based on task requirements, incorporating information gathered\n",
      "through web searches and integrating these tools into their workflow. OpenHands [15] allows agents to create code\n",
      "scripts in a human-like manner during interaction with the environment to assist in task completion. In comparison,\n",
      "Alita enables MCP creation, which provides additional benefits, including better reusability and easy environment\n",
      "management over tool creation.\n",
      "2.4 MCP\n",
      "The Model Context Protocol (MCP) is a standard proposed by Anthropic, designed to unify the connection between AI\n",
      "systems and external data sources and services. RAG-MCP [ 21] enhances the efficiency and accuracy of agents by\n",
      "retrieving the most relevant tools from a large collection, based on the task description, within the database composed\n",
      "of MCP descriptions. After tool generation, Alita wraps the generated valid tools into MCPs for subsequent use,\n",
      "facilitating reuse by itself and other agents.\n",
      "2https://openai.com/index/introducing-deep-research/\n",
      "3\n",
      "\n",
      "3 Methods\n",
      "We propose Alita, a generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal\n",
      "self-evolution to tackle diverse and complex tasks. Figure 3 illustrates the framework of Alita. In contrast to generalist\n",
      "agents that typically depend on extensive manually-designed tools and workflows [8, 9], the manager agent in Alita\n",
      "solely orchestrates the web agent using only basic tools. Through this approach, our framework enables Alita to plan\n",
      "task-specific tools through brainstorming. It then utilizes a Web Agent to search for helpful open-source libraries\n",
      "and other resources related to these tools. Leveraging the search results, Alita autonomously generates new tools and\n",
      "configures the necessary environments to enhance its capabilities and effectively solve tasks. During this process, if\n",
      "any issues arise with the newly generated tools or their environments, Alita can provide feedback and self-correct,\n",
      "improving the quality of the generated tools. Furthermore, the new tools can be encapsulated as MCP servers for future\n",
      "reuse. With the aid of MCPs, Alita can generate increasingly powerful, diverse, and complex MCPs, thus establishing a\n",
      "self-reinforcing cycle. Therefore, Alita autonomously expands its capabilities through continuous MCP integration.\n",
      "Manager Agent\n",
      "Web Agent\n",
      "Open-source\n",
      "Searching\n",
      "Script \n",
      "Generating\n",
      "Virtual Env \n",
      "Execution\n",
      "CodeReAct Loop\n",
      "Output\n",
      "MCP Box\n",
      "Encapsulate\n",
      "Question\n",
      "MCP \n",
      "Brainstorming\n",
      "Self Evolving\n",
      "MCP Creation\n",
      "Figure 3: The architecture of Alita. Upon receiving a question, the Manager Agent initiates an iterative CodeReAct\n",
      "loop to analyze tasks, identify functional gaps, and trigger MCP Brainstorming for creative synthesis. The system\n",
      "dynamically performs open-source searching, script generation, and virtual environment execution to construct task-\n",
      "related functions. Useful ones are encapsulated into reusable MCPs and stored in the MCP Box. Throughout this\n",
      "process, the Manager Agent collaborates with the Web Agent for external information retrieval and continuously\n",
      "integrates intermediate results until a final output is produced. This process enables Alita to self-evolve without relying\n",
      "on a huge hand-crafted, elaborate tools and workflows.\n",
      "3.1 Execution Pipeline\n",
      "Each task commences with the construction of an augmented prompt that incorporates the original query. The manager\n",
      "agent (Sec. 3.2) then initiates a multi-step reasoning process to address the task at hand. Throughout this process, the\n",
      "agent may query external sources via the web agent (Sec. 3.3), plan and synthesize new tools (Sec. 3.4), and execute\n",
      "them within isolated environments (Sec. 3.4.4).\n",
      "Upon successful tool generation and accurate result formulation, the corresponding script is transformed into an MCP\n",
      "and stored in the internal tool registry for future reuse. All reasoning steps, intermediate code, and final outputs are\n",
      "systematically logged to facilitate comprehensive analysis.\n",
      "3.2 Manager Agent\n",
      "The Manager Agent functions as the central coordinator within framework of Alita. When receiving a task prompt, the\n",
      "manager agent initially calls the MCP Brainstorming to determine whether additional tools are needed and which\n",
      "specific tools are required. Then the manager agent decomposes the task into subtasks and dispatches them to web\n",
      "agent or generates the required external tools to complete the subtask. When necessary, the manager agent utilizes the\n",
      "information retrieved by the web agent to generate the required new tools along with their corresponding environment\n",
      "configuration instructions. After collecting all intermediate results, the manager performs final aggregation and response\n",
      "formulation.\n",
      "4\n",
      "\n",
      "Tool Usage. In contrast to traditional systems that rely on extensive predefined toolkits, the manager agent em-\n",
      "braces Alita’s minimal philosophy by employing concise but powerful toolkits, including MCP Brainstorming,\n",
      "ScriptGeneratingTool and CodeRunningTool. Specifically, MCP Brainstorming detects functional gaps, identi-\n",
      "fies necessary supplementary tools and outlines tool specifications; ScriptGeneratingTool obtains tool specification\n",
      "outlines and then generates appropriate tools tailored to the task requirements; CodeRunningTool executes generated\n",
      "code in isolated environments and caches the output for potential MCP servers generation. These tools are intelligently\n",
      "invoked in response to the task’s evolving demands, ensuring adaptive and efficient problem-solving.\n",
      "3.3 Web Agent\n",
      "The web agent retrieves relevant information from external sources when internal knowledge is insufficient. It is\n",
      "particularly effective for tasks requiring the retrieval of domain-specific code or documentation. With a lightweight,\n",
      "text-based web interface and modular navigation tools, the web agent traverses multiple websites, extract relevant\n",
      "segments, and return reasonable URLs or raw content.\n",
      "Tool Usage. The agent utilizes SimpleTextBrowser as its web interface and page-level control tools: VisitTool,\n",
      "PageUpTool, and PageDownTool to navigate webpages. For query-based lookups, it applies GoogleSearchTool for\n",
      "open web search and GithubSearchTool to identify reusable open-source tools. This design supports real-time code\n",
      "retrieval and context-aware tool planning.\n",
      "3.4 MCP Creation Component\n",
      "To enable the creativity of the agent, we design three tools collaboratively contributing to the MCP creation process.\n",
      "3.4.1 MCP Brainstorming\n",
      "Since LLMs often exhibit overconfidence in their capabilities [ 22], we introduce MCP Brainstorming to conduct\n",
      "preliminary capability assessment by providing both the task and the description of current framework. We designed spe-\n",
      "cialized prompts to facilitate accurate self-assessment of the agent’s capabilities. Moreover, whenMCP Brainstorming\n",
      "identifies insufficient capabilities of the framework to complete the task, it provides references for tool generation to\n",
      "bridge the capability gap. This provides prior guidance for subsequent tool selection and task planning required to\n",
      "accomplish given objectives.\n",
      "3.4.2 ScriptGeneratingTool\n",
      "The ScriptGeneratingTool is a code-building utility designed for constructing external tools. It receives explicit\n",
      "subtask descriptions and suggestions for code construction from the manager agent, and potentially useful GitHub links\n",
      "obtained via the web agent, which can provide information such as README.md files or code snippets from GitHub to\n",
      "guide the script generation process. Furthermore, ScriptGeneratingTool generates the environment script to create\n",
      "the required environment for the code running and the cleaning script to clean up redundant files and environments\n",
      "generated after script execution. Therefore, ScriptGeneratingTool ensures that the generated scripts are valid,\n",
      "self-contained, and executable, making them suitable for deployment in the given task, and reusable in the future.\n",
      "3.4.3 CodeRunningTool\n",
      "The CodeRunningTool validates the functionality of the generated script by executing it within an isolated environment.\n",
      "If the execution produces the expected results, the tool is registered in the system as a reusable MCP. This process also\n",
      "supports iterative refinement, allowing for error inspection and subsequent code regeneration to improve the script’s\n",
      "performance.\n",
      "3.4.4 Environment Management\n",
      "Upon retrieving or generating a candidate tool, the system activates the environment planner module. This module\n",
      "parses the relevant repository or script metadata such as README.md, requirements.txt, and shell scripts using\n",
      "the TextInspectorTool. It extracts and validates the dependencies and setup instructions to construct an isolated\n",
      "execution profile. Subsequently, a new Conda environment is created with a unique name (typically derived from the\n",
      "task ID or a hash of the repository path), and dependencies are installed using conda install or pip install.\n",
      "All runtime environments are initialized locally in parallel, obviating the need for administrative privileges or container-\n",
      "ization technologies. This approach ensures high compatibility across various tasks while preserving the portability\n",
      "5\n",
      "\n",
      "of the system. During execution, the environment is explicitly activated prior to invoking the code interpreter, thus\n",
      "ensuring both isolation and reproducibility.\n",
      "In the event of a failure during environment initialization—due to issues such as missing packages, syntax errors in setup\n",
      "scripts, or unavailable dependencies—Alita activates an automated recovery procedure. This procedure attempts various\n",
      "fallback strategies, including relaxing version constraints or identifying the minimal set of dependencies required for\n",
      "functionality. If these recovery attempts are unsuccessful, the tool is discarded, and the failure is logged for offline\n",
      "analysis and future investigation. This enables Alita to self-correct its designed tools, thereby generating more accurate\n",
      "and robust solutions.\n",
      "4 Experiments\n",
      "4.1 Experiment Setting\n",
      "4.1.1 Benchmarks\n",
      "To evaluate the general task-handling capabilities of Alita, we conducted extensive testing across multiple agent\n",
      "benchmarks.\n",
      "GAIA [10]: GAIA is a benchmark designed to assess the capabilities of general-purpose AI assistants. It consists of\n",
      "466 real-world scenario-based questions covering daily tasks, scientific reasoning, web browsing, and tool usage. While\n",
      "these tasks are conceptually simple for humans, they are challenging for most advanced AI systems.\n",
      "Mathvista [23]: MathVista is a comprehensive benchmark designed to evaluate the mathematical reasoning capabilities\n",
      "of foundation models within visual contexts. It can effectively evaluate the model’s capabilities in visual comprehension,\n",
      "mathematical reasoning, programming, and other related skills. Due to limitations in resources, we randomly selected\n",
      "100 samples from the dataset.\n",
      "Pathvqa [24]: PathVQA is a medical visual question answering dataset. It can effectively assess the agent’s capabilities\n",
      "across multiple dimensions, including visual understanding, spatial Reasoning, medical Knowledge search or integration,\n",
      "and natural language processing. Due to limitations in resources, we randomly selected 100 samples from the dataset.\n",
      "4.1.2 Baselines\n",
      "We include a variety of baselines for comparison. For the GAIA benchmark, there are more baselines available on the\n",
      "GAIA leaderboard3.\n",
      "Octotools [9]: OctoTools is a recent framework designed to streamline multi-tool workflows in complex computational\n",
      "tasks. With over 10 standardized tool cards encapsulating various functionalities, the agent gains powerful capabilities\n",
      "to handle multi-domain tasks.\n",
      "Open Deep Research-smolagents4 [25]: Open Deep Research is an open-source agent developed under Hugging\n",
      "Face’s Smolagents project, designed to automate complex multi-step research tasks. Alita’s development is largely\n",
      "based on the framework of Open Deep Research-smolagents. However, we remove many pre-defined tools and also add\n",
      "the MCP creation component to follow the design principle of minimal predefinition and maximal self-evolution.\n",
      "AutoAgent [17]: AutoAgent is a zero-code platform designed to facilitate the creation, customization, and deployment\n",
      "of agents powered by LLMs. By providing a natural language interface, it allows users to develop multi-agent systems,\n",
      "design workflows, and integrate tools without requiring technical expertise.\n",
      "OWL [8]: OWL is an open-source, multi-agent framework built on the CAMEL-AI platform, designed to support the\n",
      "automation of complex real-world tasks through dynamic agent collaboration. OWL decomposes tasks into specialized\n",
      "sub-tasks, each of which is managed by a distinct agent type—such as UserAgents, AssistantAgents, and ToolAgents.\n",
      "A-World [12]: A-World is an open-source multi-agent system framework designed to simplify the construction,\n",
      "evaluation, and deployment of general multi-agent tasks. Through its modular design, the framework supports\n",
      "autonomous decision-making, tool usage, and collaboration among agents.\n",
      "OpenAI Deep Research5: OpenAI’s Deep Research is an advanced AI agent integrated with ChatGPT, designed to\n",
      "autonomously perform multi-step research tasks by synthesizing information from diverse online sources. This agentic\n",
      "3https://huggingface.co/spaces/gaia-benchmark/leaderboard\n",
      "4https://huggingface.co/blog/open-deep-research\n",
      "5https://openai.com/index/introducing-deep-research/\n",
      "6\n",
      "\n",
      "framework excels in generating comprehensive reports on complex topics and has shown superior performance on\n",
      "benchmarks.\n",
      "4.2 Results\n",
      "We run three rounds of testing on GAIA and achieved the best performance on the GAIA leaderboard, surpassing\n",
      "other agent systems. Alita with Claude-Sonnet-4 and GPT-4o achieves 75.15% pass@1 and 87.27% pass@3 accuracy,\n",
      "which is top-ranking on the GAIA benchmark validation dataset, outperforming many agent systems with far greater\n",
      "complexity. Alita with Claude 3.7 Sonnet + GPT-4o achieves 72.73% pass@1 and 86.06% pass@3 on GAIA, and\n",
      "further attains 74.00% and 52.00% pass@1 on the Mathvista and PathVQA benchmarks, respectively, outperforming\n",
      "Octotools and Open Deep Research by smolagents. More detailed results are shown in Table 1.\n",
      "Agent\n",
      "GAIA Mathvista PathVQA\n",
      "level1 level2 level3 total\n",
      "Alita (Claude-3.7-Sonnet, GPT-4o) (%)\n",
      "pass@1 81.13 75.58 46.15 72.73 74 52\n",
      "pass@2 88.68 80.23 53.85 78.79 - -\n",
      "pass@3 96.23 86.04 65.38 86.06 - -\n",
      "Alita (Claude-Sonnet-4, GPT-4o) (%)\n",
      "pass@1 77.36 76.74 65.38 75.15 - -\n",
      "pass@3 88.68 89.53 76.92 87.27 - -\n",
      "Baselines (%)\n",
      "Octotools - - - 18.40 68 47\n",
      "ODR-smolagents 67.92 53.49 34.62 55.15 65 42\n",
      "AutoAgent 71.70 53.49 26.92 55.15 - -\n",
      "OWL 84.91 67.44 42.31 69.09 - -\n",
      "A-World 86.79 69.77 34.62 69.70 - -\n",
      "OpenAI-DR 74.29 69.06 47.60 67.36 - -\n",
      "Table 1: Performance comparison of Alita and baseline agent systems on the GAIA, Mathvista, and PathVQA\n",
      "benchmarks. ODR-Smolagents refers to the Open Deep Research agent in the Smolagents framework. OpenAI-DR\n",
      "refers to OpenAI’s Deep Research. The table presents the accuracy at different levels of difficulty for GAIA, as well as\n",
      "the overall performance on Mathvista and PathVQA. The pass@1, pass@2, and pass@3 denote the accuracy achieved\n",
      "by running the Alita framework 1, 2, and 3 times, respectively, and selecting the best answer. Alita outperforms all\n",
      "baseline agents across the GAIA levels, achieving the highest total accuracy.\n",
      "5 Analysis\n",
      "5.1 Reuse of Alita-Generated MCPs\n",
      "5.1.1 Overview\n",
      "We collect the MCPs generated from running the GAIA dataset using Alita in conjunction with powerful models\n",
      "(Claude-3.7-Sonnet and GPT-4o). The benefits of reusing Alita-generated MCPs are two-fold. First, these MCPs can be\n",
      "reused by other agent frameworks and improve their performance since Alita, instead of human developers, designs a\n",
      "set of useful MCPs fit to GAIA by trial and error. Second, these MCPs can be reused by agents with smaller LLMs\n",
      "and significantly improve the performance. The reuse of auto-generated MCPs for agents with smaller LLMs can be\n",
      "viewed as a new way of distillation from larger LLMs. Traditionally, distillation might be fine-tuning smaller LLMs on\n",
      "data generated by larger LLMs. In comparison, the reuse of MCPs generated from agents with larger LLMs is much\n",
      "easier, cheaper, and faster than traditional distillation.\n",
      "7\n",
      "\n",
      "5.1.2 Reuse by Open Deep Research-smolagents\n",
      "We run open Deep Research-smolagents [25] on GAIA with and without Alita-generated MCPs based on GPT-4o. The\n",
      "results are presented in Table 2. From this experiment, we observe that the reuse of Alita-generated MCPs results in\n",
      "better performance compared to the base framework for all difficulty levels. This demonstrates that Alita can generate\n",
      "very useful MCPs, which can be provided to other agents, helping them enhance their capabilities and solve problems\n",
      "that would otherwise be unsolvable. Additionally, the consistent improvement across all difficulty levels indicates that\n",
      "Alita’s MCPs provide generalizable utility rather than just addressing specific edge cases in the dataset.\n",
      "Model Configuration Level 1 Level 2 Level 3 Total\n",
      "ODR-smolagents + GPT-4o(No Alita MCPs) 33.96% 29.07% 11.54% 27.88%\n",
      "ODR-smolagents + GPT-4o(With Alita MCPs) 39.62% 36.05% 15.38% 33.94%\n",
      "Table 2: Comparison of performance between ODR-smolagents with and without Alita-generated MCPs. The results\n",
      "are reported at different GAIA levels: Level 1, Level 2, Level 3, and the average. Each column corresponds to the\n",
      "performance at the respective GAIA levels. The reuse of Alita-generated MCPs can enhance the performance of other\n",
      "agents.\n",
      "5.1.3 Reuse by Base Agent on Smaller LLM\n",
      "We reuse MCPs in the base framework, i.e., ODR-smolagents [25], without the MCP creation component in Alita, and\n",
      "also with some extra pre-defined tools used in ODR-smolagents based on GPT-4o-mini. The results are presented in\n",
      "Table 3.\n",
      "Model Configuration Level 1 Level 2 Level 3 Average\n",
      "Base Framework + GPT-4o-mini (No Alita MCP) 32.08% 20.93% 3.85% 21.82%\n",
      "Base Framework + GPT-4o-mini (With Alita MCP) 39.62% 27.91% 11.54% 29.09%\n",
      "Table 3: Comparison of performance between the base framework on GPT-4o-mini, with and without Alita-generated\n",
      "MCPs. The results are reported at different GAIA levels: Level 1, Level 2, Level 3, and the average. Each column\n",
      "corresponds to the performance at the respective GAIA levels. The reuse of Alita-generated MCPs significantly\n",
      "enhances the performance of agents on smaller LLMs.\n",
      "From this experiment, we observe that the reuse of Alita-generated MCPs significantly improves performance over the\n",
      "base framework based on a smaller LLM. This is because the Alita-generated MCPs can be considered MCPs distilled\n",
      "from powerful models (Claude-3.7-Sonnet), which are made available for agents on smaller LLMs. This helps bridge\n",
      "the gap between the agents on smaller LLMs and agents on larger LLMs in certain domains, thereby enhancing its\n",
      "task-processing capabilities. Especially for Level 3, we observe a particularly dramatic improvement with the accuracy\n",
      "tripling from 3.85% to 11.54%. This substantial improvement on the most challenging problems demonstrates that\n",
      "Alita-generated MCPs are especially valuable for complex reasoning tasks where agents on smaller LLMs typically\n",
      "struggle the most. The MCPs effectively encapsulate sophisticated problem-solving capabilities that the smaller model\n",
      "can leverage without needing to develop the full reasoning chain independently.\n",
      "5.2 Alita on Smaller LLM\n",
      "We hypothesize that Alita will be even stronger with the increasing coding and reasoning capabilities of LLMs in\n",
      "the future. To validate our performance, we run Alita on GAIA using GPT-4o-mini instead of Claude-3.7-Sonnet. The\n",
      "results can be found in Table 4. Different to the experiment in Section 5.1.3, the agent doesn’t have distilled MCPs - the\n",
      "agent on GPT-4o-mini model must generate its own MCPs. The results are presented in Table 4.\n",
      "From this experiment, on one hand, we observe that Alita, after replacing the models with GPT-4o-mini, performs\n",
      "significantly worse on GAIA. This substantial performance gap highlights the critical role of the underlying models’\n",
      "coding capabilities. On the other hand, the performance of Alita increases rapidly as the capabilities of the underlying\n",
      "models improve. We can expect that with future updates to the LLMs, Alita’s performance will continue to strengthen,\n",
      "surpassing its current capabilities. The design of future generalist agents might be much simpler in the future, without\n",
      "any predefined tools and workflows for direct problem-solving. Instead, human developers might focus on designing\n",
      "modules for enabling and stimulating the creativity and evolution of generalist agents.\n",
      "8\n",
      "\n",
      "Model Configuration Level 1 Level 2 Level 3 Total\n",
      "Alita (Claude-3.7-Sonnet, GPT-4o) 81.13% 75.58% 46.15% 72.73%\n",
      "Alita (GPT-4o-mini) 54.72% 44.19% 19.23% 43.64%\n",
      "Table 4: Comparison of performance between Alita(Claude-3.7-Sonnet,GPT-4o) and Alita(GPT-4o-mini). The results\n",
      "are reported at different GAIA levels: Level 1, Level 2, Level 3, and the average. Each column corresponds to the\n",
      "performance at the respective GAIA levels. The integration of a smaller model significantly reduces the performance.\n",
      "5.3 Case Study\n",
      "To investigate Alita’s workflow when tackling tasks, we conducted a case study on its approach to solving a Level 3\n",
      "difficult problem in GAIA. The details of this process are presented in Appendix A. From the case study, we observe\n",
      "that Alita is able to perform a structured MCP brainstorming session based on the task at hand, effectively identifying\n",
      "and utilizing relevant resources to implement a feasible MCP that aids in completing the task.\n",
      "6 Conclusion\n",
      "In this work, we introduced Alita, a generalist agent designed with the principles of minimal predefinition and maximal\n",
      "self-evolution. By significantly reducing reliance on manually predefined tools and workflows for direct solving,\n",
      "Alita leverages creative, autonomous capabilities in real time, facilitating scalable agentic reasoning. Our approach\n",
      "demonstrates that simplicity in design does not undermine, but rather enhances, the performance and adaptability of\n",
      "generalist agents.\n",
      "9\n",
      "\n",
      "References\n",
      "[1] OpenAI. Introducing deep research.\n",
      "[2] Noam Kolt. Governing ai agents. arXiv preprint arXiv:2501.07913, 2025.\n",
      "[3] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su.\n",
      "Travelplanner: A benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622,\n",
      "2024.\n",
      "[4] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun\n",
      "Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in\n",
      "real computer environments. Advances in Neural Information Processing Systems, 37:52040–52094, 2024.\n",
      "[5] Hongru Wang, Rui Wang, Boyang Xue, Heming Xia, Jingtao Cao, Zeming Liu, Jeff Z. Pan, and Kam-Fai Wong.\n",
      "AppBench: Planning of multiple APIs from various APPs for complex user instruction. In Yaser Al-Onaizan,\n",
      "Mohit Bansal, and Yun-Nung Chen, editors,Proceedings of the 2024 Conference on Empirical Methods in Natural\n",
      "Language Processing, pages 15322–15336, Miami, Florida, USA, November 2024. Association for Computational\n",
      "Linguistics.\n",
      "[6] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin\n",
      "Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli\n",
      "Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao\n",
      "Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars:\n",
      "Pioneering automated gui interaction with native agents, 2025.\n",
      "[7] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the deep research.\n",
      "arXiv preprint arXiv:2502.04644, 2025.\n",
      "[8] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru\n",
      "Li, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Ping Luo, and Guohao Li. Owl: Optimized workforce learning for\n",
      "general multi-agent assistance in real-world task automation, 2025.\n",
      "[9] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octotools: An agentic framework\n",
      "with extensible tools for complex reasoning. arXiv preprint arXiv:2502.11271, 2025.\n",
      "[10] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for\n",
      "general ai assistants. In The Twelfth International Conference on Learning Representations, 2023.\n",
      "[11] Xun Jiang, Feng Li, Han Zhao, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, Yize\n",
      "Chen, et al. Long term memory: The foundation of ai self-evolution. arXiv preprint arXiv:2410.15665, 2024.\n",
      "[12] Agent Team at Ant Group. Aworld: A unified agent playground for computer and phone use tasks, 2025.\n",
      "[13] Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Zhu, Friederike Niedtner,\n",
      "Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor\n",
      "Dibia, Ahmed Awadallah, Ece Kamar, Rafah Hosn, and Saleema Amershi. Magentic-one: A generalist multi-agent\n",
      "system for solving complex tasks, 2024.\n",
      "[14] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu, and Yemin Shi.\n",
      "Autoagents: A framework for automatic agent generation. arXiv preprint arXiv:2309.17288, 2023.\n",
      "[15] Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song,\n",
      "Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. In\n",
      "The Thirteenth International Conference on Learning Representations, 2024.\n",
      "[16] Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng,\n",
      "Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762,\n",
      "2024.\n",
      "[17] Jiabin Tang, Tianyu Fan, and Chao Huang. Autoagent: A fully-automated and zero-code framework for llm agents.\n",
      "arXiv e-prints, pages arXiv–2502, 2025.\n",
      "[18] Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R Fung, Hao Peng, and Heng Ji. Craft: Customizing llms by creating\n",
      "and retrieving from specialized toolsets. arXiv preprint arXiv:2309.17428, 2023.\n",
      "[19] Zhiruo Wang, Daniel Fried, and Graham Neubig. Trove: Inducing verifiable and efficient toolboxes for solving\n",
      "programmatic tasks. arXiv preprint arXiv:2401.12869, 2024.\n",
      "[20] Cheng Qian, Chi Han, Yi R Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Tool creation for disentangling\n",
      "abstract and concrete reasoning of large language models. arXiv preprint arXiv:2305.14318, 2023.\n",
      "10\n",
      "\n",
      "[21] Tiantian Gan and Qiyao Sun. Rag-mcp: Mitigating prompt bloat in llm tool selection via retrieval-augmented\n",
      "generation. arXiv preprint arXiv:2505.03275, 2025.\n",
      "[22] Fengfei Sun, Ningke Li, Kailong Wang, and Lorenz Goette. Large language models are overconfident and amplify\n",
      "human bias. 2025.\n",
      "[23] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\n",
      "Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual\n",
      "contexts. In International Conference on Learning Representations (ICLR), 2024.\n",
      "[24] Xuehai He, Yichen Zhang, Luntian Mou, Eric P. Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical\n",
      "visual question answering. ArXiv, abs/2003.10286, 2020.\n",
      "[25] Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismäki. ‘smo-\n",
      "lagents‘: a smol library to build great agentic systems. https://github.com/huggingface/smolagents,\n",
      "2025.\n",
      "11\n",
      "\n",
      "A Detailed Case Study\n",
      "Case Study: YouTube 360 VR Video Subtitle Extraction\n",
      "Question ID: 0512426f-4d28-49f0-be77-06d05daec096\n",
      "Question: In the YouTube 360 VR video from March 2018 narrated by the voice actor of Lord of the Rings’\n",
      "Gollum, what number was mentioned by the narrator directly after dinosaurs were first shown in the video?\n",
      "Our Answer: 100000000\n",
      "Correct Answer: 100000000\n",
      "Is Correct: Yes\n",
      "Generated MCP: YouTube Video Subtitle Crawler\n",
      "Alita Workflow:\n",
      "1. MCP Brainstorming: Alita propose the development of a \"YouTube Video Subtitle Crawler\" MCP, which\n",
      "should automate the extraction of subtitles from a given YouTube video. This involves scraping the subtitles of\n",
      "the video and processing them to isolate the relevant text after the event in question.\n",
      "2. Web Agent Execution: To implement the subtitle extraction, a search is conducted in open-source\n",
      "repositories to find relevant tools that can assist in extracting YouTube video transcripts. An appropriate tool,\n",
      "the youtube-transcript-api, is identified from the following GitHub repository:\n",
      "https://github.com/jdepoix/youtube-transcript-api\n",
      "3. Manager Agent: The Manager Agent synthesizes the information from the GitHub repository and proceeds\n",
      "to write a Python function that leverages the youtube-transcript-api to retrieve the transcript of the video with\n",
      "corresponding environment setup instructions.\n",
      "The environment setup and installation steps are defined as follows:\n",
      "conda create -n youtube_transcript\n",
      "conda activate youtube_transcript\n",
      "pip install youtube-transcript-api\n",
      "The Python code to retrieve the video transcript is as follows:\n",
      "from youtube_transcript_api import YouTubeTranscriptApi\n",
      "# Initialize the API\n",
      "ytt_api = YouTubeTranscriptApi()\n",
      "# Retrieve the transcript\n",
      "video_id = ...\n",
      "transcript_list = ytt_api.list(’video_id’)\n",
      "...\n",
      "4. Manager Agent Execution: Leveraging the Python code and the established environment, the Manager\n",
      "Agent successfully packaged the YouTube Video Subtitle Crawler MCP. Subsequently, this MCP was employed\n",
      "to efficiently scrape the subtitles from the video, enabling the extraction of the relevant content. After analyzing\n",
      "the content, the correct number (100000000) mentioned by the narrator following the dinosaur scene is extracted\n",
      "from the transcript.\n",
      "5. Final Output: The number \"100000000\" is identified as the correct answer.\n",
      "B Limitations\n",
      "Alita highly relies on the coding capability of LLM. When the LLM’s coding capability is really poor, our method will\n",
      "perform worse than traditional generalist agent.\n",
      "12\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## summary ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The case study demonstrates Alita’s workflow for extracting a specific piece of information from a YouTube 360 VR video. The process begins with an MCP Brainstorming step, where Alita identifies the need for a “YouTube Video Subtitle Crawler” MCP to automate subtitle extraction. The Web Agent then searches open‑source repositories and locates the `youtube-transcript-api` library on GitHub. The Manager Agent synthesizes this information, writes a Python function that uses the API to fetch the transcript, and generates environment setup instructions (conda environment creation and pip install). Once the code is executed in the prepared environment, the Manager Agent packages the function into the MCP, which is then used to scrape the subtitles from the target video. By parsing the transcript, Alita identifies the number “100000000” mentioned immediately after the dinosaurs are first shown. This answer matches the correct answer provided in the dataset.\n",
      "\n",
      "[[ ## summary ## ]]\n",
      "Alita successfully generated a YouTube Video Subtitle Crawler MCP, executed it to retrieve the transcript of the specified 360 VR video, and extracted the correct number “100000000” mentioned by the narrator after the dinosaur scene. The workflow involved MCP brainstorming, web search for an open‑source tool, environment setup, code generation, MCP packaging, and final answer extraction.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6aa49",
   "metadata": {},
   "source": [
    "## DSPy predict - A zero vector DB example\n",
    "\n",
    "Adding an instruction to the Signature helps us to couch the LLM's reply.\n",
    "\n",
    "> Not recommended because the document will greatly clog the LLM's context window. This code just demonstrates the power of having a long context window and how to use DSPy declarative signatures with instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ac9feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vector_db = dspy.Predict(\n",
    "    dspy.Signature(\n",
    "        'document: str, question: str -> answer: str',\n",
    "        instructions='Only use the document to answer the question and nothing else.'\n",
    "    )\n",
    ")\n",
    "\n",
    "question = 'How does ALITA help LLMs to achieve autonomous reasoning?'\n",
    "response = zero_vector_db(question=question, document=doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8362814b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ALITA enables large language models (LLMs) to perform autonomous reasoning by adopting a design philosophy of **minimal predefinition and maximal self‑evolution**.  \n",
       "Key mechanisms include:\n",
       "\n",
       "1. **MCP Brainstorming** – The LLM first introspects the task, identifies missing capabilities, and proposes new *Model‑agnostic Toolchains* (MCPs) that can be built on‑the‑fly.  \n",
       "2. **Web Agent Retrieval** – It searches public code repositories and APIs to find existing libraries that can implement the proposed MCP, thereby avoiding the need for the model to write code from scratch.  \n",
       "3. **Dynamic Environment Construction** – The LLM generates the necessary environment‑setup commands (e.g., conda or pip installs) and integrates them with the retrieved code.  \n",
       "4. **Self‑Generated MCP Packaging** – The model packages the retrieved code and environment instructions into a reusable MCP, which can be invoked as a tool for the current task.  \n",
       "5. **Iterative Refinement** – If the first attempt fails, the model can regenerate the MCP or adjust its reasoning chain, effectively learning from its own failures.  \n",
       "\n",
       "By allowing the model to **create, evolve, and reuse tools in real time**, ALITA turns the LLM into an autonomous reasoner that no longer relies on a fixed set of pre‑built tools or workflows. This self‑evolving capability scales with the underlying model’s coding and reasoning power, enabling more complex, multi‑step problem solving without human‑written tool libraries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7428d0bd",
   "metadata": {},
   "source": [
    "YES!! No vector database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18cc75e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-09T21:48:12.008800]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `document` (str): \n",
      "2. `question` (str):\n",
      "Your output fields are:\n",
      "1. `answer` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## document ## ]]\n",
      "{document}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Only use the document to answer the question and nothing else.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## document ## ]]\n",
      "arXiv:2505.20286v1  [cs.AI]  26 May 2025\n",
      "ALITA : G ENERALIST AGENT ENABLING SCALABLE AGENTIC\n",
      "REASONING WITH MINIMAL PREDEFINITION AND MAXIMAL\n",
      "SELF -EVOLUTION\n",
      "Jiahao Qiu∗1, Xuan Qi∗2, Tongcheng Zhang∗3, Xinzhe Juan3,4, Jiacheng Guo1, Yifu Lu1, Yimin Wang3,4, Zixin Yao1,\n",
      "Qihan Ren3, Xun Jiang5, Xing Zhou5, Dongrui Liu3, Ling Yang1, Yue Wu1, Kaixuan Huang1, Shilong Liu1,\n",
      "Hongru Wang6, Mengdi Wang1\n",
      "1AI Lab, Princeton University 2IIIS, Tsinghua University 3Shanghai Jiao Tong University\n",
      "4University of Michigan 5Tianqiao and Chrissy Chen Institute 6The Chinese University of Hong Kong\n",
      "0.3 0.4 0.5 0.6 0.7 0.8 0.9\n",
      "Average\n",
      "Level 3\n",
      "Level 2\n",
      "Level 1\n",
      "87.3%\n",
      "76.9%\n",
      "89.5%\n",
      "88.7%\n",
      "73.3%\n",
      "57.7%\n",
      "70.1%\n",
      "86.5%\n",
      "67.4%\n",
      "47.6%\n",
      "69.1%\n",
      "74.3%\n",
      "GAIA Benchmark\n",
      "Alita manus.ai OpenAI DeepResearch\n",
      "Figure 1: Performance of Alita, manus.ai, and OpenAI DeepResearch[1]\n",
      "ABSTRACT\n",
      "Recent advances in large language models (LLMs) have enabled agents to autonomously perform\n",
      "complex, open-ended tasks. However, many existing frameworks depend heavily on manually\n",
      "predefined tools and workflows, which hinder their adaptability, scalability, and generalization\n",
      "across domains. In this work, we introduce Alita—a generalist agent designed with the principle\n",
      "of \"Simplicity is the ultimate sophistication,\" enabling scalable agentic reasoning through minimal\n",
      "predefinition and maximal self-evolution. For minimal predefinition, Alita is equipped with only one\n",
      "component for direct problem-solving, making it much simpler and neater than previous approaches\n",
      "that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances\n",
      "its potential to generalize to challenging questions, without being limited by tools. For Maximal\n",
      "self-evolution, we enable the creativity of Alita by providing a suite of general-purpose components\n",
      "to autonomously construct, refine, and reuse external capabilities by generating task-related model\n",
      "context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably,\n",
      "Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-\n",
      "purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively,\n",
      "on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. More\n",
      "details will be updated at https://github.com/CharlesQ9/Alita.\n",
      "∗ These authors contributed equally to this work.\n",
      "\n",
      "1 Introduction\n",
      "\"Simplicity is the ultimate sophistication.\" — Leonardo da Vinci\n",
      "Large language models (LLMs) have rapidly evolved from merely generating text to autonomous agents capable of\n",
      "independently planning and executing complex tasks on behalf of users with limited human oversight [ 2]. These\n",
      "capabilities have enabled a wide range of applications, ranging from travel planning [ 3], computer use [ 4, 5, 6],\n",
      "to the multi-step research tasks [ 7]. To support such diverse and demanding tasks, a new class of systems called\n",
      "generalist agents has emerged. These agents are designed to handle a wide range of domains and tasks through a unified\n",
      "architecture, allowing them to generalize beyond task-specific solutions, such as OpenAI Deep Research [1] and Manus.\n",
      "However, most of the current general-purpose agents heavily rely on large-scale manual engineering, including tediously\n",
      "designed workflows, considerable pre-defined tools, and hardcoded components [8, 9]. This reliance introduces several\n",
      "critical limitations: i) It is impractical, if not impossible, to predefine all the tools required for the wide variety of\n",
      "real-world tasks an agent might encounter (imcomplete coverage); ii) Many complex tasks require agents to creatively\n",
      "compose new tools or leverage existing ones in novel ways while pre-designed workflow and hardcoded components\n",
      "constrain this compositional flexibility and inhibit the development of adaptive behaviors ( limited creativity and\n",
      "flexibility); iii) It is not always the case that the interface or environment of different tools are compatible with the agent\n",
      "(mismatch). For example, many useful tools are not written in Python, which makes it difficult, though not entirely\n",
      "impossible, for them to be pre-connected to the mainstream agent frameworks that are primarily written in Python.\n",
      "Together, these challenges ultimately hinder the scalability, adaptability, and generalization of existing generalist agents.\n",
      "In contrast to the prevailing trend of growing complexity, we propose a radically simple design philosophy built on two\n",
      "principles: i) Minimal Predefinition: Equip the agent with only a minimal set of core capabilities, avoiding manually\n",
      "engineered components for specific tasks or modalities; ii)Maximal Self-Evolution: Empower the agent to autonomously\n",
      "create, refine, and reuse external capabilities as needed. We instantiate this vision through Alita, a generalist agent built\n",
      "with a single core capability (i.e., the web agent) and a small set of general-purpose modules that enable self-directed\n",
      "capability expansion. Specifically, we take advantage of the Model Context Protocols (MCPs) 1 which is an open\n",
      "protocol that standardizes how different systems provide context to LLMs, and empower Alita to dynamically generate,\n",
      "adapt, and reuse MCPs based on the demands of each task rather than relying on static, predefined tools. This shift from\n",
      "manually designed capabilities to on-the-fly MCP construction unlocks a new path for building agents that are simple\n",
      "yet profoundly capable.\n",
      "Youtube \n",
      "Caption \n",
      "Crawler\n",
      "Image \n",
      "CaptionerUrl Text \n",
      "Extractor\n",
      "Path \n",
      "Generalist \n",
      "Classifier\n",
      "Relevant \n",
      "Patch \n",
      "Zoomer\n",
      "Web Agent MCP Box\n",
      "Alita\n",
      "(Ours)\n",
      "Minimal Predefinition\n",
      "Maximal Self-Evolution\n",
      "Self Evolving\n",
      "Other\n",
      "Agents\n",
      " \n",
      "Manager Agent\n",
      "Manager Agent\n",
      "MCP Creation\n",
      "Large-scale Manual \n",
      "Engineering\n",
      "Traditional \n",
      "Generalist\n",
      "Agents\n",
      "Scalable Dynamic Capability\n",
      "Enhanced Creativity & Flexibility\n",
      "Cross-ecosystem Compatibility\n",
      "Incomplete Coverage\n",
      "Limited Creativity & Flexibility\n",
      "Mismatch\n",
      "Web \n",
      "Agent\n",
      "Figure 2: Comparison between Traditional Generalist Agents and Alita. Traditional generalist agents heavily rely on\n",
      "large-scale manual engineering while Alita adheres to minimal predefinition and maximal self-evolution.\n",
      "We conduct comprehensive experiments on several benchmarks to assess Alita in real-world applications, especially\n",
      "on the most popular GAIA [10]. Alita proves that simplicity is not a constraint, but a strength, and that creative agent\n",
      "behavior can emerge from a design that prioritizes autonomy over manual engineering. To sum up, our key contributions\n",
      "can be summarized as follows.\n",
      "• We propose a new agent architecture centered on minimal predefinition and maximal self-evolution, challenging\n",
      "conventional design norms in generalist agents, aiming to call for a more scalable and generalizable agent\n",
      "framework.\n",
      "1https://www.anthropic.com/news/model-context-protocol\n",
      "2\n",
      "\n",
      "• We present Alita, a generalist agent that achieves scalable agentic reasoning with a radically simple design.\n",
      "• We empirically demonstrate that Alita, despite using no complex predefined tools and workflows for specific\n",
      "tasks, outperforms many systems with significantly more handcrafted complexity on the GAIA benchmark.\n",
      "We achieve 75.15% pass@1 and 87.27% pass@3, surpassing OpenAI’s Deep Research with 67.36% pass@1\n",
      "and ranking top among all general-purpose agents.\n",
      "2 Related Works\n",
      "2.1 Generalist Agent\n",
      "The concept of a Generalist Agent aims to construct an AI agent system capable of collaboratively completing a variety\n",
      "of complex tasks in a real-world environment. OWL [ 8] introduces a method that decomposes complex tasks into\n",
      "subtasks and dynamically allocates them to worker nodes with specialized toolkits. Omne [ 11] proposes a multi-\n",
      "agent collaborative development framework, where each agent possesses an independent system structure, enabling\n",
      "autonomous learning and the storage of a comprehensive world model to build an independent understanding of the\n",
      "environment. OpenAI Deep Research 2 employs reinforcement learning for training on real-world tasks, aiming to\n",
      "provide precise and reliable research reports for knowledge-intensive tasks. A-World [12] offers a highly configurable,\n",
      "modular, and scalable simulation environment, allowing developers to flexibly define and integrate different types of AI\n",
      "agents. The Magentic-One [ 13] framework merges the Magentic and Autogen systems, distinguishing between the\n",
      "micro-level LLM-driven function generation and the macro-level multi-agent orchestration, resulting in a clearer and\n",
      "more efficient construction of agent systems. Alita, also a Generalist Agent, allows for the minimal use of predefined\n",
      "tools and workflows for direct problem-solving, yet still achieves impressive performance across diverse tasks.\n",
      "2.2 Auto Generating Agent\n",
      "Auto-generating agents aim to enhance the versatility of agents by enabling them to autonomously generate tools,\n",
      "agents, or workflows tailored to specific tasks. AutoAgents [14], for instance, generate multiple agents, each playing a\n",
      "distinct role, to handle the corresponding subtasks. OpenHands [ 15] offers an event-driven architecture that allows\n",
      "agents to interact with the environment like human developers, thereby enabling the creation of custom workflows.\n",
      "AFlow [16] redefines workflow optimization as a search problem, where each workflow consists of several nodes\n",
      "invoking large LLMs, and the optimal workflow is identified and executed through a search process. AutoAgent [17], as\n",
      "an autonomous agent operating system, permits agents to manage system-level operations and file data autonomously.\n",
      "In Alita, agents are empowered to automatically generate diverse, specialized, and highly accurate MCPs to aid in the\n",
      "completion of specific tasks, while also providing resources for future executions.\n",
      "2.3 Tool Creation\n",
      "Tool Creation enables agents to autonomously create tools to assist in task execution, either on their own or with external\n",
      "support. CRAFT [18] utilizes GPT-4 to generate a set of code snippets that function as tools, which are then retrieved\n",
      "and used in the system. TroVE [19] maintains a collection of high-level functions, which are automatically generated,\n",
      "extended, and periodically pruned to optimize program generation. CREATOR [20] decouples the abstraction of tool\n",
      "creation from the actual execution, allowing LLMs to address tasks at different levels of granularity. AutoAgent [17]\n",
      "enables agents to autonomously create new tools based on task requirements, incorporating information gathered\n",
      "through web searches and integrating these tools into their workflow. OpenHands [15] allows agents to create code\n",
      "scripts in a human-like manner during interaction with the environment to assist in task completion. In comparison,\n",
      "Alita enables MCP creation, which provides additional benefits, including better reusability and easy environment\n",
      "management over tool creation.\n",
      "2.4 MCP\n",
      "The Model Context Protocol (MCP) is a standard proposed by Anthropic, designed to unify the connection between AI\n",
      "systems and external data sources and services. RAG-MCP [ 21] enhances the efficiency and accuracy of agents by\n",
      "retrieving the most relevant tools from a large collection, based on the task description, within the database composed\n",
      "of MCP descriptions. After tool generation, Alita wraps the generated valid tools into MCPs for subsequent use,\n",
      "facilitating reuse by itself and other agents.\n",
      "2https://openai.com/index/introducing-deep-research/\n",
      "3\n",
      "\n",
      "3 Methods\n",
      "We propose Alita, a generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal\n",
      "self-evolution to tackle diverse and complex tasks. Figure 3 illustrates the framework of Alita. In contrast to generalist\n",
      "agents that typically depend on extensive manually-designed tools and workflows [8, 9], the manager agent in Alita\n",
      "solely orchestrates the web agent using only basic tools. Through this approach, our framework enables Alita to plan\n",
      "task-specific tools through brainstorming. It then utilizes a Web Agent to search for helpful open-source libraries\n",
      "and other resources related to these tools. Leveraging the search results, Alita autonomously generates new tools and\n",
      "configures the necessary environments to enhance its capabilities and effectively solve tasks. During this process, if\n",
      "any issues arise with the newly generated tools or their environments, Alita can provide feedback and self-correct,\n",
      "improving the quality of the generated tools. Furthermore, the new tools can be encapsulated as MCP servers for future\n",
      "reuse. With the aid of MCPs, Alita can generate increasingly powerful, diverse, and complex MCPs, thus establishing a\n",
      "self-reinforcing cycle. Therefore, Alita autonomously expands its capabilities through continuous MCP integration.\n",
      "Manager Agent\n",
      "Web Agent\n",
      "Open-source\n",
      "Searching\n",
      "Script \n",
      "Generating\n",
      "Virtual Env \n",
      "Execution\n",
      "CodeReAct Loop\n",
      "Output\n",
      "MCP Box\n",
      "Encapsulate\n",
      "Question\n",
      "MCP \n",
      "Brainstorming\n",
      "Self Evolving\n",
      "MCP Creation\n",
      "Figure 3: The architecture of Alita. Upon receiving a question, the Manager Agent initiates an iterative CodeReAct\n",
      "loop to analyze tasks, identify functional gaps, and trigger MCP Brainstorming for creative synthesis. The system\n",
      "dynamically performs open-source searching, script generation, and virtual environment execution to construct task-\n",
      "related functions. Useful ones are encapsulated into reusable MCPs and stored in the MCP Box. Throughout this\n",
      "process, the Manager Agent collaborates with the Web Agent for external information retrieval and continuously\n",
      "integrates intermediate results until a final output is produced. This process enables Alita to self-evolve without relying\n",
      "on a huge hand-crafted, elaborate tools and workflows.\n",
      "3.1 Execution Pipeline\n",
      "Each task commences with the construction of an augmented prompt that incorporates the original query. The manager\n",
      "agent (Sec. 3.2) then initiates a multi-step reasoning process to address the task at hand. Throughout this process, the\n",
      "agent may query external sources via the web agent (Sec. 3.3), plan and synthesize new tools (Sec. 3.4), and execute\n",
      "them within isolated environments (Sec. 3.4.4).\n",
      "Upon successful tool generation and accurate result formulation, the corresponding script is transformed into an MCP\n",
      "and stored in the internal tool registry for future reuse. All reasoning steps, intermediate code, and final outputs are\n",
      "systematically logged to facilitate comprehensive analysis.\n",
      "3.2 Manager Agent\n",
      "The Manager Agent functions as the central coordinator within framework of Alita. When receiving a task prompt, the\n",
      "manager agent initially calls the MCP Brainstorming to determine whether additional tools are needed and which\n",
      "specific tools are required. Then the manager agent decomposes the task into subtasks and dispatches them to web\n",
      "agent or generates the required external tools to complete the subtask. When necessary, the manager agent utilizes the\n",
      "information retrieved by the web agent to generate the required new tools along with their corresponding environment\n",
      "configuration instructions. After collecting all intermediate results, the manager performs final aggregation and response\n",
      "formulation.\n",
      "4\n",
      "\n",
      "Tool Usage. In contrast to traditional systems that rely on extensive predefined toolkits, the manager agent em-\n",
      "braces Alita’s minimal philosophy by employing concise but powerful toolkits, including MCP Brainstorming,\n",
      "ScriptGeneratingTool and CodeRunningTool. Specifically, MCP Brainstorming detects functional gaps, identi-\n",
      "fies necessary supplementary tools and outlines tool specifications; ScriptGeneratingTool obtains tool specification\n",
      "outlines and then generates appropriate tools tailored to the task requirements; CodeRunningTool executes generated\n",
      "code in isolated environments and caches the output for potential MCP servers generation. These tools are intelligently\n",
      "invoked in response to the task’s evolving demands, ensuring adaptive and efficient problem-solving.\n",
      "3.3 Web Agent\n",
      "The web agent retrieves relevant information from external sources when internal knowledge is insufficient. It is\n",
      "particularly effective for tasks requiring the retrieval of domain-specific code or documentation. With a lightweight,\n",
      "text-based web interface and modular navigation tools, the web agent traverses multiple websites, extract relevant\n",
      "segments, and return reasonable URLs or raw content.\n",
      "Tool Usage. The agent utilizes SimpleTextBrowser as its web interface and page-level control tools: VisitTool,\n",
      "PageUpTool, and PageDownTool to navigate webpages. For query-based lookups, it applies GoogleSearchTool for\n",
      "open web search and GithubSearchTool to identify reusable open-source tools. This design supports real-time code\n",
      "retrieval and context-aware tool planning.\n",
      "3.4 MCP Creation Component\n",
      "To enable the creativity of the agent, we design three tools collaboratively contributing to the MCP creation process.\n",
      "3.4.1 MCP Brainstorming\n",
      "Since LLMs often exhibit overconfidence in their capabilities [ 22], we introduce MCP Brainstorming to conduct\n",
      "preliminary capability assessment by providing both the task and the description of current framework. We designed spe-\n",
      "cialized prompts to facilitate accurate self-assessment of the agent’s capabilities. Moreover, whenMCP Brainstorming\n",
      "identifies insufficient capabilities of the framework to complete the task, it provides references for tool generation to\n",
      "bridge the capability gap. This provides prior guidance for subsequent tool selection and task planning required to\n",
      "accomplish given objectives.\n",
      "3.4.2 ScriptGeneratingTool\n",
      "The ScriptGeneratingTool is a code-building utility designed for constructing external tools. It receives explicit\n",
      "subtask descriptions and suggestions for code construction from the manager agent, and potentially useful GitHub links\n",
      "obtained via the web agent, which can provide information such as README.md files or code snippets from GitHub to\n",
      "guide the script generation process. Furthermore, ScriptGeneratingTool generates the environment script to create\n",
      "the required environment for the code running and the cleaning script to clean up redundant files and environments\n",
      "generated after script execution. Therefore, ScriptGeneratingTool ensures that the generated scripts are valid,\n",
      "self-contained, and executable, making them suitable for deployment in the given task, and reusable in the future.\n",
      "3.4.3 CodeRunningTool\n",
      "The CodeRunningTool validates the functionality of the generated script by executing it within an isolated environment.\n",
      "If the execution produces the expected results, the tool is registered in the system as a reusable MCP. This process also\n",
      "supports iterative refinement, allowing for error inspection and subsequent code regeneration to improve the script’s\n",
      "performance.\n",
      "3.4.4 Environment Management\n",
      "Upon retrieving or generating a candidate tool, the system activates the environment planner module. This module\n",
      "parses the relevant repository or script metadata such as README.md, requirements.txt, and shell scripts using\n",
      "the TextInspectorTool. It extracts and validates the dependencies and setup instructions to construct an isolated\n",
      "execution profile. Subsequently, a new Conda environment is created with a unique name (typically derived from the\n",
      "task ID or a hash of the repository path), and dependencies are installed using conda install or pip install.\n",
      "All runtime environments are initialized locally in parallel, obviating the need for administrative privileges or container-\n",
      "ization technologies. This approach ensures high compatibility across various tasks while preserving the portability\n",
      "5\n",
      "\n",
      "of the system. During execution, the environment is explicitly activated prior to invoking the code interpreter, thus\n",
      "ensuring both isolation and reproducibility.\n",
      "In the event of a failure during environment initialization—due to issues such as missing packages, syntax errors in setup\n",
      "scripts, or unavailable dependencies—Alita activates an automated recovery procedure. This procedure attempts various\n",
      "fallback strategies, including relaxing version constraints or identifying the minimal set of dependencies required for\n",
      "functionality. If these recovery attempts are unsuccessful, the tool is discarded, and the failure is logged for offline\n",
      "analysis and future investigation. This enables Alita to self-correct its designed tools, thereby generating more accurate\n",
      "and robust solutions.\n",
      "4 Experiments\n",
      "4.1 Experiment Setting\n",
      "4.1.1 Benchmarks\n",
      "To evaluate the general task-handling capabilities of Alita, we conducted extensive testing across multiple agent\n",
      "benchmarks.\n",
      "GAIA [10]: GAIA is a benchmark designed to assess the capabilities of general-purpose AI assistants. It consists of\n",
      "466 real-world scenario-based questions covering daily tasks, scientific reasoning, web browsing, and tool usage. While\n",
      "these tasks are conceptually simple for humans, they are challenging for most advanced AI systems.\n",
      "Mathvista [23]: MathVista is a comprehensive benchmark designed to evaluate the mathematical reasoning capabilities\n",
      "of foundation models within visual contexts. It can effectively evaluate the model’s capabilities in visual comprehension,\n",
      "mathematical reasoning, programming, and other related skills. Due to limitations in resources, we randomly selected\n",
      "100 samples from the dataset.\n",
      "Pathvqa [24]: PathVQA is a medical visual question answering dataset. It can effectively assess the agent’s capabilities\n",
      "across multiple dimensions, including visual understanding, spatial Reasoning, medical Knowledge search or integration,\n",
      "and natural language processing. Due to limitations in resources, we randomly selected 100 samples from the dataset.\n",
      "4.1.2 Baselines\n",
      "We include a variety of baselines for comparison. For the GAIA benchmark, there are more baselines available on the\n",
      "GAIA leaderboard3.\n",
      "Octotools [9]: OctoTools is a recent framework designed to streamline multi-tool workflows in complex computational\n",
      "tasks. With over 10 standardized tool cards encapsulating various functionalities, the agent gains powerful capabilities\n",
      "to handle multi-domain tasks.\n",
      "Open Deep Research-smolagents4 [25]: Open Deep Research is an open-source agent developed under Hugging\n",
      "Face’s Smolagents project, designed to automate complex multi-step research tasks. Alita’s development is largely\n",
      "based on the framework of Open Deep Research-smolagents. However, we remove many pre-defined tools and also add\n",
      "the MCP creation component to follow the design principle of minimal predefinition and maximal self-evolution.\n",
      "AutoAgent [17]: AutoAgent is a zero-code platform designed to facilitate the creation, customization, and deployment\n",
      "of agents powered by LLMs. By providing a natural language interface, it allows users to develop multi-agent systems,\n",
      "design workflows, and integrate tools without requiring technical expertise.\n",
      "OWL [8]: OWL is an open-source, multi-agent framework built on the CAMEL-AI platform, designed to support the\n",
      "automation of complex real-world tasks through dynamic agent collaboration. OWL decomposes tasks into specialized\n",
      "sub-tasks, each of which is managed by a distinct agent type—such as UserAgents, AssistantAgents, and ToolAgents.\n",
      "A-World [12]: A-World is an open-source multi-agent system framework designed to simplify the construction,\n",
      "evaluation, and deployment of general multi-agent tasks. Through its modular design, the framework supports\n",
      "autonomous decision-making, tool usage, and collaboration among agents.\n",
      "OpenAI Deep Research5: OpenAI’s Deep Research is an advanced AI agent integrated with ChatGPT, designed to\n",
      "autonomously perform multi-step research tasks by synthesizing information from diverse online sources. This agentic\n",
      "3https://huggingface.co/spaces/gaia-benchmark/leaderboard\n",
      "4https://huggingface.co/blog/open-deep-research\n",
      "5https://openai.com/index/introducing-deep-research/\n",
      "6\n",
      "\n",
      "framework excels in generating comprehensive reports on complex topics and has shown superior performance on\n",
      "benchmarks.\n",
      "4.2 Results\n",
      "We run three rounds of testing on GAIA and achieved the best performance on the GAIA leaderboard, surpassing\n",
      "other agent systems. Alita with Claude-Sonnet-4 and GPT-4o achieves 75.15% pass@1 and 87.27% pass@3 accuracy,\n",
      "which is top-ranking on the GAIA benchmark validation dataset, outperforming many agent systems with far greater\n",
      "complexity. Alita with Claude 3.7 Sonnet + GPT-4o achieves 72.73% pass@1 and 86.06% pass@3 on GAIA, and\n",
      "further attains 74.00% and 52.00% pass@1 on the Mathvista and PathVQA benchmarks, respectively, outperforming\n",
      "Octotools and Open Deep Research by smolagents. More detailed results are shown in Table 1.\n",
      "Agent\n",
      "GAIA Mathvista PathVQA\n",
      "level1 level2 level3 total\n",
      "Alita (Claude-3.7-Sonnet, GPT-4o) (%)\n",
      "pass@1 81.13 75.58 46.15 72.73 74 52\n",
      "pass@2 88.68 80.23 53.85 78.79 - -\n",
      "pass@3 96.23 86.04 65.38 86.06 - -\n",
      "Alita (Claude-Sonnet-4, GPT-4o) (%)\n",
      "pass@1 77.36 76.74 65.38 75.15 - -\n",
      "pass@3 88.68 89.53 76.92 87.27 - -\n",
      "Baselines (%)\n",
      "Octotools - - - 18.40 68 47\n",
      "ODR-smolagents 67.92 53.49 34.62 55.15 65 42\n",
      "AutoAgent 71.70 53.49 26.92 55.15 - -\n",
      "OWL 84.91 67.44 42.31 69.09 - -\n",
      "A-World 86.79 69.77 34.62 69.70 - -\n",
      "OpenAI-DR 74.29 69.06 47.60 67.36 - -\n",
      "Table 1: Performance comparison of Alita and baseline agent systems on the GAIA, Mathvista, and PathVQA\n",
      "benchmarks. ODR-Smolagents refers to the Open Deep Research agent in the Smolagents framework. OpenAI-DR\n",
      "refers to OpenAI’s Deep Research. The table presents the accuracy at different levels of difficulty for GAIA, as well as\n",
      "the overall performance on Mathvista and PathVQA. The pass@1, pass@2, and pass@3 denote the accuracy achieved\n",
      "by running the Alita framework 1, 2, and 3 times, respectively, and selecting the best answer. Alita outperforms all\n",
      "baseline agents across the GAIA levels, achieving the highest total accuracy.\n",
      "5 Analysis\n",
      "5.1 Reuse of Alita-Generated MCPs\n",
      "5.1.1 Overview\n",
      "We collect the MCPs generated from running the GAIA dataset using Alita in conjunction with powerful models\n",
      "(Claude-3.7-Sonnet and GPT-4o). The benefits of reusing Alita-generated MCPs are two-fold. First, these MCPs can be\n",
      "reused by other agent frameworks and improve their performance since Alita, instead of human developers, designs a\n",
      "set of useful MCPs fit to GAIA by trial and error. Second, these MCPs can be reused by agents with smaller LLMs\n",
      "and significantly improve the performance. The reuse of auto-generated MCPs for agents with smaller LLMs can be\n",
      "viewed as a new way of distillation from larger LLMs. Traditionally, distillation might be fine-tuning smaller LLMs on\n",
      "data generated by larger LLMs. In comparison, the reuse of MCPs generated from agents with larger LLMs is much\n",
      "easier, cheaper, and faster than traditional distillation.\n",
      "7\n",
      "\n",
      "5.1.2 Reuse by Open Deep Research-smolagents\n",
      "We run open Deep Research-smolagents [25] on GAIA with and without Alita-generated MCPs based on GPT-4o. The\n",
      "results are presented in Table 2. From this experiment, we observe that the reuse of Alita-generated MCPs results in\n",
      "better performance compared to the base framework for all difficulty levels. This demonstrates that Alita can generate\n",
      "very useful MCPs, which can be provided to other agents, helping them enhance their capabilities and solve problems\n",
      "that would otherwise be unsolvable. Additionally, the consistent improvement across all difficulty levels indicates that\n",
      "Alita’s MCPs provide generalizable utility rather than just addressing specific edge cases in the dataset.\n",
      "Model Configuration Level 1 Level 2 Level 3 Total\n",
      "ODR-smolagents + GPT-4o(No Alita MCPs) 33.96% 29.07% 11.54% 27.88%\n",
      "ODR-smolagents + GPT-4o(With Alita MCPs) 39.62% 36.05% 15.38% 33.94%\n",
      "Table 2: Comparison of performance between ODR-smolagents with and without Alita-generated MCPs. The results\n",
      "are reported at different GAIA levels: Level 1, Level 2, Level 3, and the average. Each column corresponds to the\n",
      "performance at the respective GAIA levels. The reuse of Alita-generated MCPs can enhance the performance of other\n",
      "agents.\n",
      "5.1.3 Reuse by Base Agent on Smaller LLM\n",
      "We reuse MCPs in the base framework, i.e., ODR-smolagents [25], without the MCP creation component in Alita, and\n",
      "also with some extra pre-defined tools used in ODR-smolagents based on GPT-4o-mini. The results are presented in\n",
      "Table 3.\n",
      "Model Configuration Level 1 Level 2 Level 3 Average\n",
      "Base Framework + GPT-4o-mini (No Alita MCP) 32.08% 20.93% 3.85% 21.82%\n",
      "Base Framework + GPT-4o-mini (With Alita MCP) 39.62% 27.91% 11.54% 29.09%\n",
      "Table 3: Comparison of performance between the base framework on GPT-4o-mini, with and without Alita-generated\n",
      "MCPs. The results are reported at different GAIA levels: Level 1, Level 2, Level 3, and the average. Each column\n",
      "corresponds to the performance at the respective GAIA levels. The reuse of Alita-generated MCPs significantly\n",
      "enhances the performance of agents on smaller LLMs.\n",
      "From this experiment, we observe that the reuse of Alita-generated MCPs significantly improves performance over the\n",
      "base framework based on a smaller LLM. This is because the Alita-generated MCPs can be considered MCPs distilled\n",
      "from powerful models (Claude-3.7-Sonnet), which are made available for agents on smaller LLMs. This helps bridge\n",
      "the gap between the agents on smaller LLMs and agents on larger LLMs in certain domains, thereby enhancing its\n",
      "task-processing capabilities. Especially for Level 3, we observe a particularly dramatic improvement with the accuracy\n",
      "tripling from 3.85% to 11.54%. This substantial improvement on the most challenging problems demonstrates that\n",
      "Alita-generated MCPs are especially valuable for complex reasoning tasks where agents on smaller LLMs typically\n",
      "struggle the most. The MCPs effectively encapsulate sophisticated problem-solving capabilities that the smaller model\n",
      "can leverage without needing to develop the full reasoning chain independently.\n",
      "5.2 Alita on Smaller LLM\n",
      "We hypothesize that Alita will be even stronger with the increasing coding and reasoning capabilities of LLMs in\n",
      "the future. To validate our performance, we run Alita on GAIA using GPT-4o-mini instead of Claude-3.7-Sonnet. The\n",
      "results can be found in Table 4. Different to the experiment in Section 5.1.3, the agent doesn’t have distilled MCPs - the\n",
      "agent on GPT-4o-mini model must generate its own MCPs. The results are presented in Table 4.\n",
      "From this experiment, on one hand, we observe that Alita, after replacing the models with GPT-4o-mini, performs\n",
      "significantly worse on GAIA. This substantial performance gap highlights the critical role of the underlying models’\n",
      "coding capabilities. On the other hand, the performance of Alita increases rapidly as the capabilities of the underlying\n",
      "models improve. We can expect that with future updates to the LLMs, Alita’s performance will continue to strengthen,\n",
      "surpassing its current capabilities. The design of future generalist agents might be much simpler in the future, without\n",
      "any predefined tools and workflows for direct problem-solving. Instead, human developers might focus on designing\n",
      "modules for enabling and stimulating the creativity and evolution of generalist agents.\n",
      "8\n",
      "\n",
      "Model Configuration Level 1 Level 2 Level 3 Total\n",
      "Alita (Claude-3.7-Sonnet, GPT-4o) 81.13% 75.58% 46.15% 72.73%\n",
      "Alita (GPT-4o-mini) 54.72% 44.19% 19.23% 43.64%\n",
      "Table 4: Comparison of performance between Alita(Claude-3.7-Sonnet,GPT-4o) and Alita(GPT-4o-mini). The results\n",
      "are reported at different GAIA levels: Level 1, Level 2, Level 3, and the average. Each column corresponds to the\n",
      "performance at the respective GAIA levels. The integration of a smaller model significantly reduces the performance.\n",
      "5.3 Case Study\n",
      "To investigate Alita’s workflow when tackling tasks, we conducted a case study on its approach to solving a Level 3\n",
      "difficult problem in GAIA. The details of this process are presented in Appendix A. From the case study, we observe\n",
      "that Alita is able to perform a structured MCP brainstorming session based on the task at hand, effectively identifying\n",
      "and utilizing relevant resources to implement a feasible MCP that aids in completing the task.\n",
      "6 Conclusion\n",
      "In this work, we introduced Alita, a generalist agent designed with the principles of minimal predefinition and maximal\n",
      "self-evolution. By significantly reducing reliance on manually predefined tools and workflows for direct solving,\n",
      "Alita leverages creative, autonomous capabilities in real time, facilitating scalable agentic reasoning. Our approach\n",
      "demonstrates that simplicity in design does not undermine, but rather enhances, the performance and adaptability of\n",
      "generalist agents.\n",
      "9\n",
      "\n",
      "References\n",
      "[1] OpenAI. Introducing deep research.\n",
      "[2] Noam Kolt. Governing ai agents. arXiv preprint arXiv:2501.07913, 2025.\n",
      "[3] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su.\n",
      "Travelplanner: A benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622,\n",
      "2024.\n",
      "[4] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun\n",
      "Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in\n",
      "real computer environments. Advances in Neural Information Processing Systems, 37:52040–52094, 2024.\n",
      "[5] Hongru Wang, Rui Wang, Boyang Xue, Heming Xia, Jingtao Cao, Zeming Liu, Jeff Z. Pan, and Kam-Fai Wong.\n",
      "AppBench: Planning of multiple APIs from various APPs for complex user instruction. In Yaser Al-Onaizan,\n",
      "Mohit Bansal, and Yun-Nung Chen, editors,Proceedings of the 2024 Conference on Empirical Methods in Natural\n",
      "Language Processing, pages 15322–15336, Miami, Florida, USA, November 2024. Association for Computational\n",
      "Linguistics.\n",
      "[6] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin\n",
      "Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli\n",
      "Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao\n",
      "Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars:\n",
      "Pioneering automated gui interaction with native agents, 2025.\n",
      "[7] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the deep research.\n",
      "arXiv preprint arXiv:2502.04644, 2025.\n",
      "[8] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru\n",
      "Li, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Ping Luo, and Guohao Li. Owl: Optimized workforce learning for\n",
      "general multi-agent assistance in real-world task automation, 2025.\n",
      "[9] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octotools: An agentic framework\n",
      "with extensible tools for complex reasoning. arXiv preprint arXiv:2502.11271, 2025.\n",
      "[10] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for\n",
      "general ai assistants. In The Twelfth International Conference on Learning Representations, 2023.\n",
      "[11] Xun Jiang, Feng Li, Han Zhao, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, Yize\n",
      "Chen, et al. Long term memory: The foundation of ai self-evolution. arXiv preprint arXiv:2410.15665, 2024.\n",
      "[12] Agent Team at Ant Group. Aworld: A unified agent playground for computer and phone use tasks, 2025.\n",
      "[13] Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Zhu, Friederike Niedtner,\n",
      "Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor\n",
      "Dibia, Ahmed Awadallah, Ece Kamar, Rafah Hosn, and Saleema Amershi. Magentic-one: A generalist multi-agent\n",
      "system for solving complex tasks, 2024.\n",
      "[14] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu, and Yemin Shi.\n",
      "Autoagents: A framework for automatic agent generation. arXiv preprint arXiv:2309.17288, 2023.\n",
      "[15] Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song,\n",
      "Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. In\n",
      "The Thirteenth International Conference on Learning Representations, 2024.\n",
      "[16] Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng,\n",
      "Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762,\n",
      "2024.\n",
      "[17] Jiabin Tang, Tianyu Fan, and Chao Huang. Autoagent: A fully-automated and zero-code framework for llm agents.\n",
      "arXiv e-prints, pages arXiv–2502, 2025.\n",
      "[18] Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R Fung, Hao Peng, and Heng Ji. Craft: Customizing llms by creating\n",
      "and retrieving from specialized toolsets. arXiv preprint arXiv:2309.17428, 2023.\n",
      "[19] Zhiruo Wang, Daniel Fried, and Graham Neubig. Trove: Inducing verifiable and efficient toolboxes for solving\n",
      "programmatic tasks. arXiv preprint arXiv:2401.12869, 2024.\n",
      "[20] Cheng Qian, Chi Han, Yi R Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Tool creation for disentangling\n",
      "abstract and concrete reasoning of large language models. arXiv preprint arXiv:2305.14318, 2023.\n",
      "10\n",
      "\n",
      "[21] Tiantian Gan and Qiyao Sun. Rag-mcp: Mitigating prompt bloat in llm tool selection via retrieval-augmented\n",
      "generation. arXiv preprint arXiv:2505.03275, 2025.\n",
      "[22] Fengfei Sun, Ningke Li, Kailong Wang, and Lorenz Goette. Large language models are overconfident and amplify\n",
      "human bias. 2025.\n",
      "[23] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\n",
      "Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual\n",
      "contexts. In International Conference on Learning Representations (ICLR), 2024.\n",
      "[24] Xuehai He, Yichen Zhang, Luntian Mou, Eric P. Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical\n",
      "visual question answering. ArXiv, abs/2003.10286, 2020.\n",
      "[25] Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismäki. ‘smo-\n",
      "lagents‘: a smol library to build great agentic systems. https://github.com/huggingface/smolagents,\n",
      "2025.\n",
      "11\n",
      "\n",
      "A Detailed Case Study\n",
      "Case Study: YouTube 360 VR Video Subtitle Extraction\n",
      "Question ID: 0512426f-4d28-49f0-be77-06d05daec096\n",
      "Question: In the YouTube 360 VR video from March 2018 narrated by the voice actor of Lord of the Rings’\n",
      "Gollum, what number was mentioned by the narrator directly after dinosaurs were first shown in the video?\n",
      "Our Answer: 100000000\n",
      "Correct Answer: 100000000\n",
      "Is Correct: Yes\n",
      "Generated MCP: YouTube Video Subtitle Crawler\n",
      "Alita Workflow:\n",
      "1. MCP Brainstorming: Alita propose the development of a \"YouTube Video Subtitle Crawler\" MCP, which\n",
      "should automate the extraction of subtitles from a given YouTube video. This involves scraping the subtitles of\n",
      "the video and processing them to isolate the relevant text after the event in question.\n",
      "2. Web Agent Execution: To implement the subtitle extraction, a search is conducted in open-source\n",
      "repositories to find relevant tools that can assist in extracting YouTube video transcripts. An appropriate tool,\n",
      "the youtube-transcript-api, is identified from the following GitHub repository:\n",
      "https://github.com/jdepoix/youtube-transcript-api\n",
      "3. Manager Agent: The Manager Agent synthesizes the information from the GitHub repository and proceeds\n",
      "to write a Python function that leverages the youtube-transcript-api to retrieve the transcript of the video with\n",
      "corresponding environment setup instructions.\n",
      "The environment setup and installation steps are defined as follows:\n",
      "conda create -n youtube_transcript\n",
      "conda activate youtube_transcript\n",
      "pip install youtube-transcript-api\n",
      "The Python code to retrieve the video transcript is as follows:\n",
      "from youtube_transcript_api import YouTubeTranscriptApi\n",
      "# Initialize the API\n",
      "ytt_api = YouTubeTranscriptApi()\n",
      "# Retrieve the transcript\n",
      "video_id = ...\n",
      "transcript_list = ytt_api.list(’video_id’)\n",
      "...\n",
      "4. Manager Agent Execution: Leveraging the Python code and the established environment, the Manager\n",
      "Agent successfully packaged the YouTube Video Subtitle Crawler MCP. Subsequently, this MCP was employed\n",
      "to efficiently scrape the subtitles from the video, enabling the extraction of the relevant content. After analyzing\n",
      "the content, the correct number (100000000) mentioned by the narrator following the dinosaur scene is extracted\n",
      "from the transcript.\n",
      "5. Final Output: The number \"100000000\" is identified as the correct answer.\n",
      "B Limitations\n",
      "Alita highly relies on the coding capability of LLM. When the LLM’s coding capability is really poor, our method will\n",
      "perform worse than traditional generalist agent.\n",
      "12\n",
      "\n",
      "[[ ## question ## ]]\n",
      "How does ALITA help LLMs to achieve autonomous reasoning?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## answer ## ]]\n",
      "ALITA enables large language models (LLMs) to perform autonomous reasoning by adopting a design philosophy of **minimal predefinition and maximal self‑evolution**.  \n",
      "Key mechanisms include:\n",
      "\n",
      "1. **MCP Brainstorming** – The LLM first introspects the task, identifies missing capabilities, and proposes new *Model‑agnostic Toolchains* (MCPs) that can be built on‑the‑fly.  \n",
      "2. **Web Agent Retrieval** – It searches public code repositories and APIs to find existing libraries that can implement the proposed MCP, thereby avoiding the need for the model to write code from scratch.  \n",
      "3. **Dynamic Environment Construction** – The LLM generates the necessary environment‑setup commands (e.g., conda or pip installs) and integrates them with the retrieved code.  \n",
      "4. **Self‑Generated MCP Packaging** – The model packages the retrieved code and environment instructions into a reusable MCP, which can be invoked as a tool for the current task.  \n",
      "5. **Iterative Refinement** – If the first attempt fails, the model can regenerate the MCP or adjust its reasoning chain, effectively learning from its own failures.  \n",
      "\n",
      "By allowing the model to **create, evolve, and reuse tools in real time**, ALITA turns the LLM into an autonomous reasoner that no longer relies on a fixed set of pre‑built tools or workflows. This self‑evolving capability scales with the underlying model’s coding and reasoning power, enabling more complex, multi‑step problem solving without human‑written tool libraries.  \n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e6bf1",
   "metadata": {},
   "source": [
    "# 2. Programmatic Signatures and how they integrate with the broader LLM ecosystem\n",
    "In general, you will have to use DSPy for any (or only the final) LLM centric operation because it is focused on LLM prompting. Every other operation (tool, vector database, etc.) can come from any other framework!\n",
    "\n",
    "> We use LlamaIndex to provide vector indexing capabilities here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0c961",
   "metadata": {},
   "source": [
    "Creating a vector database to ingest our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a812d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "index = VectorStoreIndex(docs, embed_model=Settings.embed_model)\n",
    "\n",
    "base_retriever = index.as_retriever(similarity_top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90c6788c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = base_retriever.retrieve(question)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "    context = dspy.InputField(desc=\"May contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"Often between 1-10 sentences.\")\n",
    "\n",
    "class RewriteQuestion(dspy.Signature):\n",
    "    question = dspy.InputField()\n",
    "    rewritten_questions: list[str] = dspy.OutputField(\n",
    "        desc=\"Decompose this question into sub questions or rewrite the original user question if necessary to improve retrieval from a vector database. Otherwise return the original question.\"\n",
    "    )\n",
    "    \n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.retriever = base_retriever\n",
    "        self.rewriter = dspy.Predict(RewriteQuestion)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.consolidate_answer = dspy.Predict(\n",
    "            dspy.Signature(\n",
    "                'original_question: str, sub_answers: list[str] -> consolidated_answer:str',\n",
    "                instructions=\"Consolidate the sub answers into a coherent answer within a few paragraphs that answers the original question.\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def query_rewrite(self, question: str):\n",
    "        return self.rewriter(question=question)\n",
    "    \n",
    "    def forward(self, question: str):\n",
    "        question_rewrite = self.query_rewrite(question)\n",
    "        sub_answers = []\n",
    "        for q in tqdm(question_rewrite.rewritten_questions):\n",
    "            print(f\"\\n----\\nProcessing question: {q}\")            \n",
    "            context = self.retriever.retrieve(q) #the LlamaIndex component\n",
    "            sub_answer = self.generate_answer(context=context, question=q)\n",
    "            print(f\"\\nAnswer to {q}: {sub_answer}\")\n",
    "            print(f\"\\nSub question answer reasoning: {sub_answer.reasoning}\\n----\\n\")\n",
    "            sub_answers.append(sub_answer)\n",
    "        prediction = self.consolidate_answer(original_question=question, sub_answers=sub_answers)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8209169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8b328c4f5a4015bd62e750489b3177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "Processing question: How does an agent become autonomous with Atila?\n",
      "\n",
      "Answer to How does an agent become autonomous with Atila?: Prediction(\n",
      "    reasoning='The question asks for a concise explanation of how an agent achieves autonomy using the Alita framework. Based on the provided documents, Alita’s autonomy comes from its minimal predefinition of tools and workflows, a manager that orchestrates planning and execution, and a self‑evolution mechanism that allows the agent to adapt and improve its reasoning over time. The answer should capture these key points in a short, factoid style.',\n",
      "    answer='Alita achieves autonomy by using a minimal set of predefined tools and workflows, a manager that orchestrates planning and execution, and a self‑evolution mechanism that lets the agent adapt and improve its reasoning over time.'\n",
      ")\n",
      "\n",
      "Sub question answer reasoning: The question asks for a concise explanation of how an agent achieves autonomy using the Alita framework. Based on the provided documents, Alita’s autonomy comes from its minimal predefinition of tools and workflows, a manager that orchestrates planning and execution, and a self‑evolution mechanism that allows the agent to adapt and improve its reasoning over time. The answer should capture these key points in a short, factoid style.\n",
      "----\n",
      "\n",
      "\n",
      "----\n",
      "Processing question: Does Alita use the Model Context Protocol?\n",
      "\n",
      "Answer to Does Alita use the Model Context Protocol?: Prediction(\n",
      "    reasoning='The provided documents describe Alita as a generalist agent that relies on minimal predefinition and maximal self‑evolution. None of the excerpts mention the Model Context Protocol, nor do they indicate that Alita incorporates or depends on such a protocol. Therefore, based on the available evidence, Alita does not use the Model Context Protocol.',\n",
      "    answer='No, Alita does not use the Model Context Protocol.'\n",
      ")\n",
      "\n",
      "Sub question answer reasoning: The provided documents describe Alita as a generalist agent that relies on minimal predefinition and maximal self‑evolution. None of the excerpts mention the Model Context Protocol, nor do they indicate that Alita incorporates or depends on such a protocol. Therefore, based on the available evidence, Alita does not use the Model Context Protocol.\n",
      "----\n",
      "\n",
      "\n",
      "----\n",
      "Processing question: Does Alita write its own tools?\n",
      "\n",
      "Answer to Does Alita write its own tools?: Prediction(\n",
      "    reasoning='The cited documents describe Alita as a generalist agent that achieves scalable reasoning with a “radically simple design.”  It explicitly states that Alita “uses no complex predefined tools and workflows for specific tasks” and relies on minimal predefinition and self‑evolution rather than generating new tools.  Therefore, Alita does not write its own tools.',\n",
      "    answer='No, Alita does not write its own tools; it operates with minimal predefinition and self‑evolution without generating new tools.'\n",
      ")\n",
      "\n",
      "Sub question answer reasoning: The cited documents describe Alita as a generalist agent that achieves scalable reasoning with a “radically simple design.”  It explicitly states that Alita “uses no complex predefined tools and workflows for specific tasks” and relies on minimal predefinition and self‑evolution rather than generating new tools.  Therefore, Alita does not write its own tools.\n",
      "----\n",
      "\n",
      "\n",
      "----\n",
      "Processing question: Where are the tools deployed?\n",
      "\n",
      "Answer to Where are the tools deployed?: Prediction(\n",
      "    reasoning='The question asks where the tools are deployed in the Alita system. The provided documents describe that Alita’s manager agent is responsible for tool usage and that it employs concise toolkits such as MCP Brainstorming, ScriptGeneratingTool, and CodeRunningTool. Therefore, the tools are deployed within the manager agent component of Alita.',\n",
      "    answer='They are deployed inside Alita’s manager agent, which orchestrates the use of the concise toolkits (MCP Brainstorming, ScriptGeneratingTool, CodeRunningTool, etc.).'\n",
      ")\n",
      "\n",
      "Sub question answer reasoning: The question asks where the tools are deployed in the Alita system. The provided documents describe that Alita’s manager agent is responsible for tool usage and that it employs concise toolkits such as MCP Brainstorming, ScriptGeneratingTool, and CodeRunningTool. Therefore, the tools are deployed within the manager agent component of Alita.\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "engine = RAG()\n",
    "pred = engine(\n",
    "    \"How Agent become autonomous with Atila? Alita use Model context protocol? It write own tools? Tools deploy where?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c56ea0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alita achieves autonomy by relying on a very small set of predefined tools and workflows, while a manager component orchestrates planning and execution. The agent’s design is intentionally minimal, allowing it to adapt and improve its reasoning over time through a self‑evolution mechanism.  \n",
       "\n",
       "It does **not** employ the Model Context Protocol, nor does it generate its own tools. Instead, Alita operates with a concise toolkit that is built into the system.  \n",
       "\n",
       "The tools are deployed inside Alita’s manager agent, which handles the use of the available toolkits such as MCP Brainstorming, ScriptGeneratingTool, and CodeRunningTool. This manager coordinates the agent’s interactions with those tools to carry out tasks autonomously."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(pred.consolidated_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca0db390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-08-09T21:54:44.187729]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `original_question` (str): \n",
      "2. `sub_answers` (list[str]):\n",
      "Your output fields are:\n",
      "1. `consolidated_answer` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## original_question ## ]]\n",
      "{original_question}\n",
      "\n",
      "[[ ## sub_answers ## ]]\n",
      "{sub_answers}\n",
      "\n",
      "[[ ## consolidated_answer ## ]]\n",
      "{consolidated_answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Consolidate the sub answers into a coherent answer within a few paragraphs that answers the original question.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## original_question ## ]]\n",
      "How Agent become autonomous with Atila? Alita use Model context protocol? It write own tools? Tools deploy where?\n",
      "\n",
      "[[ ## sub_answers ## ]]\n",
      "[Prediction(\n",
      "    reasoning='The question asks for a concise explanation of how an agent achieves autonomy using the Alita framework. Based on the provided documents, Alita’s autonomy comes from its minimal predefinition of tools and workflows, a manager that orchestrates planning and execution, and a self‑evolution mechanism that allows the agent to adapt and improve its reasoning over time. The answer should capture these key points in a short, factoid style.',\n",
      "    answer='Alita achieves autonomy by using a minimal set of predefined tools and workflows, a manager that orchestrates planning and execution, and a self‑evolution mechanism that lets the agent adapt and improve its reasoning over time.'\n",
      "), Prediction(\n",
      "    reasoning='The provided documents describe Alita as a generalist agent that relies on minimal predefinition and maximal self‑evolution. None of the excerpts mention the Model Context Protocol, nor do they indicate that Alita incorporates or depends on such a protocol. Therefore, based on the available evidence, Alita does not use the Model Context Protocol.',\n",
      "    answer='No, Alita does not use the Model Context Protocol.'\n",
      "), Prediction(\n",
      "    reasoning='The cited documents describe Alita as a generalist agent that achieves scalable reasoning with a “radically simple design.”  It explicitly states that Alita “uses no complex predefined tools and workflows for specific tasks” and relies on minimal predefinition and self‑evolution rather than generating new tools.  Therefore, Alita does not write its own tools.',\n",
      "    answer='No, Alita does not write its own tools; it operates with minimal predefinition and self‑evolution without generating new tools.'\n",
      "), Prediction(\n",
      "    reasoning='The question asks where the tools are deployed in the Alita system. The provided documents describe that Alita’s manager agent is responsible for tool usage and that it employs concise toolkits such as MCP Brainstorming, ScriptGeneratingTool, and CodeRunningTool. Therefore, the tools are deployed within the manager agent component of Alita.',\n",
      "    answer='They are deployed inside Alita’s manager agent, which orchestrates the use of the concise toolkits (MCP Brainstorming, ScriptGeneratingTool, CodeRunningTool, etc.).'\n",
      ")]\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## consolidated_answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## consolidated_answer ## ]]\n",
      "Alita achieves autonomy by relying on a very small set of predefined tools and workflows, while a manager component orchestrates planning and execution. The agent’s design is intentionally minimal, allowing it to adapt and improve its reasoning over time through a self‑evolution mechanism.  \n",
      "\n",
      "It does **not** employ the Model Context Protocol, nor does it generate its own tools. Instead, Alita operates with a concise toolkit that is built into the system.  \n",
      "\n",
      "The tools are deployed inside Alita’s manager agent, which handles the use of the available toolkits such as MCP Brainstorming, ScriptGeneratingTool, and CodeRunningTool. This manager coordinates the agent’s interactions with those tools to carry out tasks autonomously.  \n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4d830",
   "metadata": {},
   "source": [
    "# 3. Creating agents with DSPy\n",
    "Agents in DSPy require tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4f34855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAgentSignature(dspy.Signature):\n",
    "    question: str = dspy.InputField()\n",
    "    history: dspy.History = dspy.InputField()\n",
    "    answer: str = dspy.OutputField()\n",
    "\n",
    "class AskForMoreInfo(dspy.Signature):\n",
    "    question = dspy.InputField()\n",
    "    response = dspy.OutputField()\n",
    "\n",
    "def ask_for_clarification_tool(question: str):\n",
    "    \"\"\"Use this tool if the user's question is unclear. This tool prompts the user for more information\"\"\"\n",
    "    clarification = dspy.Predict(\n",
    "        AskForMoreInfo(\n",
    "            question=question,\n",
    "            instructions=\"The user has asked an ambiguous question. Ask the user for clarifications to the question.\"\n",
    "        )\n",
    "    )\n",
    "    return clarification.response\n",
    "\n",
    "def query_alita_knowledge_base(question: str):\n",
    "    \"\"\"Use this tool to query the knowledge base on Alita.\"\"\"\n",
    "    engine = RAG()\n",
    "    pred = engine(question)\n",
    "    return pred.consolidated_answer\n",
    "\n",
    "history = dspy.History(messages=[])\n",
    "agent = dspy.ReAct(RAGAgentSignature, tools=[query_alita_knowledge_base, ask_for_clarification_tool,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a77c71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent(question=\"jkok\", history=history)\n",
    "history.messages.append({\"question\": \"jkok\", **response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "414370dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Could you please clarify what you mean by “jkok”? Are you asking for information about a topic, a location, or something else?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87c4f6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2733447cd4da41718c57e208aa0f5504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "Processing question: What steps are required for an agent to become autonomous using Alita?\n",
      "\n",
      "Answer to What steps are required for an agent to become autonomous using Alita?: Prediction(\n",
      "    reasoning='The question asks for the steps an agent must follow to become autonomous when using Alita. Based on the provided documents, Alita’s design emphasizes minimal predefinition, self‑evolution, and scalable reasoning. The autonomous workflow therefore involves: (1) giving Alita a high‑level goal or task description; (2) letting Alita generate a plan or set of sub‑tasks (MCPs) without relying on pre‑built tools; (3) executing the plan while monitoring outcomes; (4) using feedback to refine and evolve the plan or internal strategies; and (5) iterating this cycle until the goal is achieved. These steps capture the core autonomous loop enabled by Alita.',\n",
      "    answer='1. Provide Alita with a high‑level goal or task description.  \\n2. Let Alita generate a plan (MCPs) using its minimal‑predefinition approach.  \\n3. Execute the plan while monitoring results.  \\n4. Use feedback to refine and evolve the plan or internal strategies.  \\n5. Iterate the cycle until the goal is achieved, achieving full autonomy.'\n",
      ")\n",
      "\n",
      "Sub question answer reasoning: The question asks for the steps an agent must follow to become autonomous when using Alita. Based on the provided documents, Alita’s design emphasizes minimal predefinition, self‑evolution, and scalable reasoning. The autonomous workflow therefore involves: (1) giving Alita a high‑level goal or task description; (2) letting Alita generate a plan or set of sub‑tasks (MCPs) without relying on pre‑built tools; (3) executing the plan while monitoring outcomes; (4) using feedback to refine and evolve the plan or internal strategies; and (5) iterating this cycle until the goal is achieved. These steps capture the core autonomous loop enabled by Alita.\n",
      "----\n",
      "\n",
      "\n",
      "----\n",
      "Processing question: What does it mean for an agent to be autonomous in Alita?\n",
      "\n",
      "Answer to What does it mean for an agent to be autonomous in Alita?: Prediction(\n",
      "    reasoning='The question asks for a concise definition of “autonomous” as used in the context of the Alita agent. From the cited documents, Alita is described as a generalist agent that can “independently plan and execute complex tasks on behalf of users with limited human oversight” and “achieves scalable agentic reasoning with minimal predefinition and maximal self‑evolution.” Thus, autonomy here refers to the agent’s ability to self‑direct its reasoning, planning, and execution without relying on extensive pre‑designed tools or continuous human intervention.',\n",
      "    answer='In Alita, an agent is autonomous when it can independently plan, reason, and execute complex tasks on its own, requiring only minimal human oversight and no extensive pre‑defined tools or workflows.'\n",
      ")\n",
      "\n",
      "Sub question answer reasoning: The question asks for a concise definition of “autonomous” as used in the context of the Alita agent. From the cited documents, Alita is described as a generalist agent that can “independently plan and execute complex tasks on behalf of users with limited human oversight” and “achieves scalable agentic reasoning with minimal predefinition and maximal self‑evolution.” Thus, autonomy here refers to the agent’s ability to self‑direct its reasoning, planning, and execution without relying on extensive pre‑designed tools or continuous human intervention.\n",
      "----\n",
      "\n",
      "\n",
      "----\n",
      "Processing question: What are the prerequisites for agent autonomy in Alita?\n",
      "\n",
      "Answer to What are the prerequisites for agent autonomy in Alita?: Prediction(\n",
      "    reasoning='The Alita paper emphasizes that its agentic autonomy is achieved through a design that relies on **minimal predefinition** (no extensive, task‑specific tools or workflows) and **maximal self‑evolution** (the agent learns and adapts during operation). These two elements—keeping the initial setup simple while allowing the agent to grow its capabilities autonomously—are presented as the prerequisites for achieving scalable, generalist agentic reasoning in Alita.',\n",
      "    answer='Alita’s agent autonomy requires:\\n1. **Minimal predefinition** – no complex, hand‑crafted tools or workflows for specific tasks.  \\n2. **Maximal self‑evolution** – the agent must be able to learn and adapt its own strategies during operation.  \\nThese two conditions enable a generalist agent to reason and act autonomously across diverse tasks.'\n",
      ")\n",
      "\n",
      "Sub question answer reasoning: The Alita paper emphasizes that its agentic autonomy is achieved through a design that relies on **minimal predefinition** (no extensive, task‑specific tools or workflows) and **maximal self‑evolution** (the agent learns and adapts during operation). These two elements—keeping the initial setup simple while allowing the agent to grow its capabilities autonomously—are presented as the prerequisites for achieving scalable, generalist agentic reasoning in Alita.\n",
      "----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "An agent becomes autonomous with Alita by following a simple, self‑driven loop that relies on two core principles: **minimal predefinition** and **maximal self‑evolution**.\n",
       "\n",
       "1. **Minimal predefinition** – The agent is given only a high‑level goal or task description, without any hand‑crafted tools, workflows, or detailed instructions. This keeps the initial setup lightweight and allows the agent to discover how to solve the problem on its own.\n",
       "\n",
       "2. **Self‑generated planning** – Using Alita’s internal reasoning engine, the agent produces a plan (often a set of modular cognitive processes, or MCPs) that outlines the steps needed to reach the goal. Because the plan is generated on‑the‑fly, the agent can adapt it to the specifics of the task.\n",
       "\n",
       "3. **Execution and monitoring** – The agent carries out the plan while continuously monitoring outcomes. It observes the results of each action and checks whether the intermediate objectives are being met.\n",
       "\n",
       "4. **Feedback‑driven refinement** – Based on the monitoring data, the agent refines its plan or internal strategies. This iterative adjustment allows the agent to learn from successes and failures, improving its performance over time.\n",
       "\n",
       "5. **Iteration until completion** – The cycle of planning, executing, monitoring, and refining repeats until the original goal is achieved. Throughout this process, the agent requires only minimal human oversight, demonstrating true autonomy.\n",
       "\n",
       "In short, an agent becomes autonomous with Alita by starting with a broad goal, letting Alita generate and execute a self‑adaptable plan, and iteratively refining its approach through continuous feedback—all while operating with minimal pre‑defined tools and maximal self‑evolution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clarified_question = \"Sorry I accidentally sent that message. Here's the question I intended to ask: How does an agent become autonomous with Alita?\"\n",
    "response = agent(question=clarified_question, history=history)\n",
    "display(Markdown(response.answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ea017",
   "metadata": {},
   "source": [
    "# 4. Prompt Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "276ff191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from llama_cloud_services import LlamaParse\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "    num_workers=4,       # if multiple files passed, split in `num_workers` API calls\n",
    "    verbose=True,\n",
    "    language=\"en\",       # optionally define a language, default=en\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"anthropic-sonnet-3.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4e03384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae185d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 9cdf89a4-1ad1-49ef-9836-5f025a799b91\n"
     ]
    }
   ],
   "source": [
    "result = await parser.aparse(\"../pdf/resume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a118b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_documents = result.get_markdown_documents(split_by_page=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09329a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_vector_store_index = VectorStoreIndex.from_documents(markdown_documents)\n",
    "\n",
    "# Save index to a filepath for easy loading.\n",
    "resume_vector_store_index.storage_context.persist(persist_dir=\"../resume_storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b4545d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ../resume_storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ../resume_storage/index_store.json.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"../resume_storage\")\n",
    "resume_vector_store_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ba68f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_retriever = resume_vector_store_index.as_retriever()\n",
    "\n",
    "class ResumeRAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resume_retriever = resume_retriever\n",
    "        self.response_synthesizer = dspy.ChainOfThought('question, contexts -> answer')\n",
    "    def forward(self, question: str):\n",
    "        contexts = self.resume_retriever.retrieve(question)\n",
    "        return self.response_synthesizer(question=question, contexts=contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b51db654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Titus overhauled Toyota Tsusho’s sales‑planning system by redesigning the formatting of sales planning files and building a centralized, automated forecasting tool in VBA. He applied operational‑research forecasting techniques such as exponential smoothing and Holt‑Winters, which helped Toyota’s automotive distributors cut stock overflows and improved long‑term sales forecast accuracy by roughly 25 %."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resume_engine = ResumeRAG()\n",
    "response = resume_engine(\"What did Titus do in Toyota Tsusho?\")\n",
    "display(Markdown(response.answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f5905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
